---
layout: presentation
title: Materializations + Incremental models
lesson: 207
---

class: title, center, middle
# dbt Materializations

---


# What is a materialization?

- A wrapper for your SQL
    - Includes all the boilerplate DDL/DML: `create`, `alter`, `drop`, ...

- An abstraction on top of the database
    - Does this model already exists in the database? What if it's materialized 
    as a table, but it currently exists as a view?
    - What if Redshift uses `alter table` for views, but Snowflake uses `alter view`?
    - What if we want to build this model incrementally?
        - Does it have the same structure as the existing version?
        - What data is "new"?

---

# Standard materializations

.left-column-66[

- Table
- View
- Ephemeral
- Incremental

### You can write your own!

- Just special Jinja macros, calling other macros
- Trickiest part: cross-database support
- E.g. ["insert by period"](https://github.com/fishtown-analytics/dbt-utils#materializations)

]

.right-column-33[
<img src="/lessons/img/{{page.lesson}}/material-houses.png" class="img-center">
]

---

# Standard materializations

.left-column-66[

- Table
- View
- Ephemeral
- .highlight[Incremental]

### You can write your own!

- Just special Jinja macros, calling other macros
- Trickiest part: cross-database support
- E.g. ["insert by period"](https://github.com/fishtown-analytics/dbt-utils#materializations)

]

.right-column-33[
<img src="/lessons/img/{{page.lesson}}/material-houses.png" class="img-center">
]

---

class: title, center, middle
# Incremental models

`where updated_at >= '{{site.time|slice: 0, 10}}'`

---

# Why?

<img src="/lessons/img/{{page.lesson}}/fr-run-timing.png" class="img-center">

---

# Why not?

**Performance first:** Incremental modeling is an optimization problem, 
specific to _your_ database. It requires a deep understanding of the 
performance features we've been discussing.

**Quiet failure mode:** The implementation is straightforward, the 
implications are complex and subtle. You need to think in advance about all 
the ways that things can go wrong.

---

# When?

.left-column[
### If you're...
- Just starting out
- Relatively small data (< 10mm rows, ~10 GB / table)
- Able to query views with low latency
- Building table models in < 90s seconds

### _don't worry about it!_
]

--

.right-column[
.left-column[
### If you're...
- Maturing as an organization
- Modeling a lot of unchanging historical data (> TB)
- Seeing table models take 5+ minutes to build
]

.right-column[
<img src="/lessons/img/{{page.lesson}}/rodin-thinker.png" style="border-radius: 50%;" class="img-center">
]
]   

---

# Who?

.left-column[
### Great candidates
- Sources
    - Immutable event streams (append-only, no updates)
    - Reliable `updated_at` field if any updates
- Fact table structure (tall + skinny)
]
--
.right-column[
### Not-so-great
- Transformations that involve:
    - Joins
    - Slowly changing dimensions (SCDs)
    - Window functions
    - High proportion of **late-arriving facts**
]
    
???
Talk through each of these:
- Joins: update cadence between join tables may differ
- SCDs: too many updates! just rebuild from scratch each time
- Window functions: you'll be calculating on top of _part_, never _whole_
- Late-arriving facts: we're going to talk about this!

---

class: subtitle, center, middle
# How?

--

### ...simple vs. correct?
### ...to make it performant on my database?

---

### [The Discourse](https://discourse.getdbt.com/t/on-the-limits-of-incrementality/303)

<img src="/lessons/img/{{page.lesson}}/incr-discourse-stats.png" class="img-center">
&nbsp;
<img src="/lessons/img/{{page.lesson}}/incr-discourse-intro.png" class="img-center">

---

# "The big easy"

.dense-text[
_Keep it simple:_ This model rolls up **events** into **sessions**. A session 
contains one or more events with the same `session_id`.

{% raw %}
```sql
{{config(
    materialized = 'incremental',
    unique_key = 'session_id'
)}}

with events as (

    select * from {{ref('events')}}
    
    {% if is_incremental %}
    where event_timestamp >= (select max(session_start) from {{this}})
    {% endif %}

),

-- rest of model --
```
{% endraw %}
]

---

# Let's dissect: special Jinja variables

### {% raw %}`{{this}}`{% endraw %}

Represents the currently existing database object mapped to this model.

### `is_incremental()`

Checks four conditions:
1. Does this model already exist as an object in the database? (Yes!)
2. Is the database object a table? (Yes!)
3. Is this model configured with an incremental materialization? (Yes!)
4. Was the `--full-refresh` flag passed to this `dbt run`? (No!)

???
If all of the conditions are met, `is_incremental()` returns `true`.

---

# How does this actually work?

<img src="/lessons/img/{{page.lesson}}/how-deeper.png" class="img-center">

How is this idempotent?

- At any point, you could `run --full-refresh` and get the "true" table.
- The goal of incremental models is to _approximate_ the "true" table in a fraction
of the runtime

---

# Under the hood: database specifics

What's the DDL/DML that dbt is running?

- `delete` + `insert` ("upsert") on Redshift and a special Snowflake "strategy"
- `merge` is default BigQuery + default Snowflake "strategy"

Basically:

- Match **new** and **existing** records on their `unique_key`
- Delete/update matched **existing** records with **new** ones
- Insert unmatched **new** records

---

# For best results

- The source `events` model is sorted by `event_timestamp`
- The model is sorted by `session_start`

```sql
with events as (

    select * from dev_jerco.events
    
    where event_timestamp >= (select max(session_start) from dev_jerco.sessions)

),

-- rest of model --
```

- Redshift: It's a good idea to distribute the `sessions` model on `session_id`
    - dbt will `delete` from the existing table using an implicit inner join

---

class: subtitle, center, middle
# How things fall apart

---

### Imagine some sessionization logic

.left-column[
```sql
sessionized as (

    select
    
        session_id,
        user_id,
        count(*) as events_in_session
        
    from events
    group by 1

),

...
```
]

.right-column[
```sql
indexed as (

    select *,
    
         row_number() over (
            partition by user_id
            order by event_timestamp
        ) as session_index
    
    from sessionized

)

select * from indexed
```
]

---

# Late-arriving facts

<img src="/lessons/img/{{page.lesson}}/how-late-arriving.png" class="img-center">

--

The session is split across our `where` filter. In the simple approach,
we would replace the first half-session (computed from first half of events)
with the second half-session.

How can we correctly calculate the _entire_ session?

---

# How common?

.left-column-66[
#### Percent of overall events, by days between firing + collection
<img src="/lessons/img/{{page.lesson}}/prop-late-arriving.png" class="img-center">
]

.right-column-33[
- 2.5 months of Snowplow data, 285mm events
- 99.82% arrive within 1 hr of firing
- 99.88% within 24 hr
- .highlight[99.93% within 72 hr]
]

---

# "Close enough & performant"

<img src="/lessons/img/{{page.lesson}}/how-close-enough.png" class="img-center">

- _Always_ recalculate the 3 previous days
- Empirically this will account for 99.93% of all events on the first try
- Full refresh on a weekly basis

---

# "Always correct & slow"

<img src="/lessons/img/{{page.lesson}}/how-always-correct.png" class="img-center">

- If a user has a new event, recalculate _all_ sessions for that user
- This works with window functions! But it's **_slowwwww_**...

---

# Too clever by half

<img src="/lessons/img/{{page.lesson}}/how-too-clever.png" class="img-center">

- If a user has a new session, pull the user's _most recent session only_,
and perform relative calculations
- This takes some hard thinking! E.g. [Segment package](https://github.com/fishtown-analytics/segment/blob/master/macros/cross-adapter-modeling/sessionization/segment_web_sessions.sql#L75)

---

# Another approach: overwrite partitions

- Recalculate all data for one day and just replace it entirely
    - **Pro:** Very efficient! Filter on partition field, save a join step
    - **Con:** No matching on a `unique_key` = no guarantee of uniqueness

- This is the default on Big Data tech (e.g. Spark), and we've also implemented
as an optional strategy for BigQuery

---

# It's the modeling, silly

- Keep the inputs and transformations of your incremental models as
singular, simple, and immutable as possible.
    - Slowly changing dimensions, like a `product_name` that the company
    regularly rebrands? Join from a `dim` table.
    - Window functions for quality-of-life counters? Fix it in post
    (downstream table). This is how our Snowplow package calculates user's `session_index`.

- Rethink! Incrementality introduces a level of complexity (for you and for
the database) that is **necessarily a trade-off**.

---

class: subtitle, middle, center
### Made it :)

{% include options/next-presentation.html %}
