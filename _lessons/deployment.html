---
layout: presentation
title: Deployment
estimated_teaching_time: 15
---

class: title, center, middle
# Deployment

---

# Ecosystem wins

Modern, modular data stack includes:
- dbt for analytics engineering
- Data warehouse
- EL (Stitch/Fivetran/...)
- BI (Looker/Mode/...)
- What else?

---

# Non-development dbt

In two senses:
- Who is a read-only user?
- What does CI/CD mean to _you_?

???
We often tell analysts to ask their neighborhood DevOps person!

---

# Earlier: Automating dbt runs
You need to run dbt on a schedule to keep your data up to date.

Does your automated run:
* Use the latest version of my project? → **continuous deployment**
* Have monitoring and alerting if something goes wrong?
* Have a way to debug failures?
* Build & test your code every time you want to merge changes ("deploy to production")? →  **continuous integration**

We use the scheduler in [dbt Cloud](cloud.getdbt.com) to run dbt in production.

---

# Alternatives to dbt Cloud

- Standalone, general-purpose orchestration tools
    - Airflow, Jenkins, Luigi, ...
- Tightly coupled with other pieces of data stack
    - GitLab CI/CD, Fivetran-hosted dbt
- Analytics in a box
    - e.g. Postgres + cron + Heroku + ELK

---

# Why?
- Preexisting architecture
- Integration with custom data ingestion
- HIPAA compliance on a budget
- **Advanced configuration + control**
    - dbt Cloud can have more compelling answers here!

---

class: subtitle, center, middle
# Future art

---

# Many environments

.left-column[
- Prod, dev, staging, QA, oh my...
- Each env needs:
    - one Snowflake database / BigQuery project
    - one set of credentials
    - one dbt Cloud project
- Code may need to differ dynamically across envs
]

--

.right-column-33[
Possible today with...
- _dbt?_ **Yes!** Many targets
- _dbt Cloud?_ **Yes**
- _downside?_ Configuration is through UI, not code
]

---

# Blue-green deploys

.left-column[
- Run tests on source data
- Run all models into a QA schema
- Run tests on QA models
- If all tests pass, promote QA schema to production

How?
- Redshift/Postgres: `alter schema ... rename`
- Snowflake: [zero-copy clones](https://docs.snowflake.com/en/sql-reference/sql/create-clone.html)
- [BigQuery](https://youtu.be/jGwUonA3mDg?t=94), more creatively
]

--

.right-column-33[
Possible today with...
- _dbt?_ Yes, many macros/operations
- _dbt Cloud?_ Yes
- _downside?_ Complex workflows can't be configured as code, visualized
]

---

# How smart is your CI?

.left-column[
- "Build on PR" jobs
    - Select _only_ models whose _code_ has changed in that PR
    - _Much_ faster, more efficient
- Q's: child models? YML changes?
]

--

.right-column-33[
Possible today with...
- _dbt?_ [Can be hacked](https://www.youtube.com/watch?v=m-QlIVss0UA), better answers forthcoming
- _dbt Cloud?_ No
]

---

# Artifact persistence

.left-column[
For example...
- Push the docs `index.html` into S3/GCS for iframe embedding in Confluence
- Integrate with Datadog, PagerDuty
- Longitudinal analysis of `run_results.json`
    - Test failures by week
    - Job runtime as overall data volume increases
]

--

.right-column-33[
Possible today with...
- _dbt?_ Yes, with ever-more-stable APIs
- _dbt Cloud?_ Not really
]

---

class: subtitle

# Knowledge check

You should:
- Understand some reasons why companies opt to use dbt _without_ dbt Cloud

{% include options/last_slide.html %}
