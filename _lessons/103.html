---
layout: presentation
title: dbt Fundamentals
lesson: 103
estimated_teaching_time: 45
estimated_working_session: 60
teaching_method: live demo + slides. Questions encouraged.
---

# dbt Fundamentals

???
Note to teacher:
- This presentation should be accompanied by re-doing the tutorial live.
- It comes after a number of lessons that are one-sided
- Ask the audience some questions / encourage participation. There will be some students who know all this already, so give them the opportunity to participate by helping answer questions.
- If people ask questions that are related to future lessons, jot them down on a shared place so you can cross them off throughout the rest of the course.

Speaker track:
- In the pre-work, you created your first dbt project.
- We're going to go through this pre-work again, build apply some concepts to it to try to build a common language (a lingua franca if you will).
- Please ask questions as we go, we need to make sure we're all on a solid foundation, as we'll continue to build on this foundation throughout the

---
layout: false
# The four ingredients for dbt

???
To use dbt, you need four things:

--

1. A data warehouse
???
* We're using Snowflake, your org might be using BigQuery / Redshift (or heaven forbid, spark)
* Flip to Snowflake console and show the data

--

    * that can run SQL
    * with data already loaded in to it

???
Questions to prompt:
- How does data get into the warehouse?

--
2. A dbt project
--

    * created with the `init` command / `initialize` button
    * a directory with a `dbt_project.yml` file + other `.sql` and `.yml` files
    * a project contains **models**

???
Create the project again/show off the models
Quick side step into models though...

--
## Models
* Models are `select` statements that transform data.
* Stored in the `models` directory, and have a `.sql` extension
???
See if anyone wants to have a go at explaining a model

--


???

---
# The four ingredients for dbt
1. A data warehouse
    * that can run SQL
    * with data already loaded in to it
2. A dbt project
    * created with the `init` command / button
    * a directory with a `dbt_project.yml` file + other `.sql` and `.yml` files
    * a project contains **models**
3. A connection to your warehouse:
--

    * dbt CLI: a profile (that matches your the `profile:` in your `dbt_project.yml` file)
    * dbt Cloud: a connection
    * contains credentials to connect to your warehouse and a **target schema**

???
* Explain why this lives outside your project:
    * creds are sensitive and shouldn't be version controlled
    * target schemas should be different for each user, so shouldn't be hardcoded in a project
* Explain what a target schema is
* For CLI users — they might want more info on targets/profiles
---
# The four ingredients for dbt
1. A data warehouse
    * that can run SQL
    * with data already loaded in to it
2. A dbt project
    * created with the `init` command / button
    * a directory with a `dbt_project.yml` file + other `.sql` and `.yml` files
    * a project contains **models**
3. A connection to your warehouse:
    * dbt CLI: a profile (that matches your the `profile:` in your `dbt_project.yml` file)
    * dbt Cloud: a connection
    * contains credentials to connect to your warehouse and a **target schema**
4. A command

--
    * an instruction, issued from the command line
    * e.g. `dbt run`

???
* Run `dbt run`!
* Explain what dbt is doing (wraps the select statement in DDL and runs it)
* Show where to see the logs


Good knowledge check questions:
* How did dbt connect to the warehouse? A: connection / profile
* How did dbt know what schema to use? A: the target schema
* What happens if you rerun dbt? A: no downtime (this looks different on each warehouse)


---

# The four ingredients for dbt
1. A data warehouse
    * that can run SQL
    * with data already loaded in to it
2. A dbt project
    * created with the `init` command / button
    * a directory with a `dbt_project.yml` file + other `.sql` and `.yml` files
    * a project contains **models**
3. A connection to your warehouse:
    * dbt CLI: a profile (that matches your the `profile:` in your `dbt_project.yml` file)
    * dbt Cloud: a connection
    * contains credentials to connect to your warehouse and a **target schema**
4. A command
    * an instruction, issued from the command line
    * e.g. `dbt run`
--

When you execute the `run` **command** dbt connects to your **data warehouse** (via a **profile**/**connection**) and builds the **models** in your **dbt project** as **relations** in your **target schema**.

---
# Building dbt models
Models are `select` statements.

When you execute the `run` **command** dbt builds **models** as **relations** in your **target schema**.

The way that dbt builds a model depends on the **materialization** you **configure** for a model.
???
Notes to teacher:
* Switching this to just building models since we should have laid down some of those concepts already
---
# Materializations

???
* What is a materialization?

--
* `materializations` are "build" strategies
* Effectively the SQL that your `select` statement gets wrapped in
* Models can be materialized as `views` and `tables` (we'll learn more later!)
* You need to **configure** a model's materialization

???
* Try changing a materialization
* We're going to talk a lot more about materializations tomorrow
---
# Configurations

???
* What is a configuration?
--
* `configurations` are model settings
* They can be set in your `dbt_project.yml` file, _and_ in your model file.
* Configurations are applied _hierarchically_
* Example configurations:
    * `materialized='view'`
    * `tags=['nightly', 'hourly']`
    * `enabled=True`

???
* Show changing configurations, esp. examples that highlight hierarchical nature
---
# Building dbt models
Models are `select` statements.

When you execute the `run` **command** dbt builds **models** as **relations** in your **target schema**.

The way that dbt builds a model depends on the **materialization** you **configure** for a model.

---

# Breathe!

???
Stop for questions at this point, make sure everyone is following.

---

# Let's do it again!
## This time with a better query!

???
Instructions to teacher:
* Copy the query from the tutorial and run it

---
layout: false
# Creating dependencies
* It's often a good idea to split up models
* Use the `ref` function to do build dependencies between models
* `ref` **compiles** to the correct table name → this allows us to work in environments.

???
* Demonstrate building dependencies
* Also demonstrate running one model at a time
---
# Building dbt models
Models are `select` statements.

When you execute the `run` **command** dbt builds **models** as **relations** in your **target schema**.

The way that dbt builds a model depends on the **materialization** you **configure** for a model.

The order that dbt builds models is determined by the `ref` function.
???

---
# Testing dbt models

???
* Add some tests to your models
* Demonstrate running one test at a time
--
**Tests** are assertions you make about your **models**.

When you execute the `test` **command** dbt iterates through the schema files (`models/**.yml`) to construct `select` queries.

These queries return `0` when your assertion is true, otherwise the test fails.


---
# Other helpful commands
CLI:
* `dbt --help` (and `dbt run --help`)
* `dbt --version`
* `dbt debug`
* `dbt compile`

???
* Demo each command — for CLI users, it's worth realizing that these are pretty standard CLI commands!

---
# Knowledge check

You should be comfortable with these words:
.left-column[
* dbt project
* models
* target schema
* materializations
]
.right-column[
* configs
* refs
* configurations
* tests
]

<br/>
You should be able to:
* Create new models (that depend on other models)
* Run your models (all of them / one model at a time)
* Check the SQL that's running
* Configure your models (from the `dbt_project.yml` file / within a model)
* Add tests to your models
* Run your tests (all of them / one model at a time)

<br/>
🙋‍♀️ Please let us know if you want to go over any of these

???
Teacher notes:
* Use this as a checklist that you've covered everything off
* Emphasize that we're going to launch off from here — use the lunch time to make sure that everyone is following so far.
* For those who do understand, there's a working session
---

# Working session
We just got some payment data into our warehouse:

```sql
select * from raw.stripe.payment
>>>
| ID | orderID | paymentMethod | AMOUNT | CREATED    |
|----|---------|---------------|--------|------------|
| 1  | 1       | credit_card   | 1000   | 2018-01-01 |
| 2  | 2       | credit_card   | 2000   | 2018-01-02 |
| 3  | 3       | coupon        | 100    | 2018-01-04 |
| 4  | 4       | coupon        | 2500   | 2018-01-05 |
| 5  | 5       | bank_transfer | 1700   | 2018-01-05 |

```
???
Notes on the data (for teachers, hopefully the students discover some of these)
* singular table name
* no underscores in column names
* cased column names
* amount is in cents
* `created` vs. `created_date`

---
name: working-session
# Working session
We just got some payment data into our warehouse:

```sql
select * from raw.stripe.payment
```

1. Build an `orders` model with the following fields:
    * `order_id`
    * `customer_id`
    * `amount` (hint: this has to come from payments)
2. Add a new field to the `customers` model:
    * `customers.lifetime_value`: the total amount a customer has spent at `jaffle_shop`
3. [Bonus] On the `orders` model, add subtotals for each `payment_method`, e.g.:
    * `credit_card_amount`
    * `coupon_amount`
    * etc.

???
Discussion points (leading questions for the next session)

Where did our data come from? How do we know that?
* All of it was in the `raw` database
* Grouped into schemas based on the source

When building your models:
* What did you name the files?
* How did you organize them in your models directory?
* Did you test these models? Did you document them?
