---
layout: presentation
title: Techniques of the trade
---

class: title, center, middle
# (dbt) Techniques of the trade


{% raw %}

???
We've already learned the tools of the trade (dbt), but now we are learning the techniques.
We're going to go through a series of problems that analytics engineers often have to solve, and discuss how we would typically solve them. Feel free to jump in with suggested techniques at any point.

Notes for teacher:
* This presentation has _a lot_ in it. The goal of this lesson is to show students what they _can_ do with dbt/give them the resources to solve these problems in the future.
* Also use this time to answer any unsolved problems from earlier in the course
* Take a collaborative approach: for each problem, see if any students can suggest a solution

---

# My source data keeps changing

---


### On Monday:
| order_id | status  | created_at | updated_at |
|----------|---------|------------|------------|
| 1        | pending | 2020-01-01 | 2020-02-20 |

### On Tuesday:
| order_id | status  | created_at | updated_at |
|----------|---------|------------|------------|
| 1        | shipped | 2020-01-01 | 2020-02-21 |

???
Imagine you have an orders table where the status field can be overwritten as the order is processed. One day you query the table and get back this result, but the next day you get this result.

You can no longer answer:
- When did that order ship?
- How long did it take to change status?

--

## What we want:

| order_id | status  | updated_at | dbt_valid_from | dbt_valid_to |
|----------|---------|------------|----------------|--------------|
| 1        | pending | 2020-01-01 | 2020-01-01     | 2020-01-02   |
| 1        | shipped | 2020-01-02 | 2020-01-02     | null         |

???
We want to turn our **mutable** data into **immutable data**

---

# How do I make my data immutable?

| order_id | status  | updated_at | dbt_valid_from | dbt_valid_to |
|----------|---------|------------|----------------|--------------|
| 1        | pending | 2019-01-01 | 2019-01-01     | 2019-01-02   |
| 1        | shipped | 2019-01-02 | 2019-01-02     | null         |

--

## Technique: Snapshots

---



# Anatomy of a snapshot
A `.sql` file in the `snapshots` directory:
.dense-text[
```sql
-- snapshots/orders_snapshot.sql
{% snapshot orders_snapshot %}

    {{
        config(
          target_database='analytics',
          target_schema='snapshots',
          unique_key='id',
          strategy='timestamp',
          updated_at='updated_at',
        )
    }}

    -- Pro-Tip: Use sources in snapshots!
    select * from {{ source('jaffle_shop', 'orders') }}

{% endsnapshot %}
```
]

???
- Snapshots are simple select statements which define a dataset. They live in the snapshots directory, and are enclosed in Jinja tags

---
# Anatomy of a snapshot
From the command line:
```bash
$ dbt snapshot

```

???
- Every time you run the dbt snapshot command, dbt will run your select statement and check if the dataset has changed compared to its last known state.
- If the records have changed, then dbt will create a new entry in the snapshot table for the new state of the record. If there are net-new records in the source dataset, then dbt will create an initial entry for the record.

--
.caption[
[Learn more about snapshots](https://docs.getdbt.com/docs/snapshots)
]

---
# I have a long case statement
--

```sql
-- models/orders.sql
select
    order_id,
    case
        when country_code = 'AU' then 'Australia'
        when country_code = 'UK' then 'United Kingdom'
        when country_code = 'US' then 'United States'
        ...
    end as country_name

from {{ ref('orders') }}
```

--

## Technique: seeds
???
Seeds are CSV files that you can `ref` in your project

---
# Anatomy of a seed

`data/country_codes.csv`:
```txt
country_code,country_name
AU,Australia
UK,United Kingdom
US,United States
```
--
Command:
```bash
$ dbt seed
```
---
# Anatomy of a seed

Downstream model:
```sql
-- models/orders.sql
select
    orders.order_id,
    country_codes.country_name

from {{ ref('orders') }} as orders

left join {{ ref('country_codes') }} as country_codes
    on orders.country_code = country_codes.country_code

```

???
* Seeds live in the `data` folder as `.csv` files
* Load them with the `dbt seed` command
* You can `ref` them like any other model
* Appropriate for version controlling business logic, not for loading huge datasets

--

.caption[
  [Learn more about seeds](https://docs.getdbt.com/docs/seeds)
]

---

# Other use cases for seeds
* Email addresses to exclude
* List of employee accounts

---

## I need to run some models hourly, and some models nightly.
--

### Technique: tags!
---
.dense-text[
In dbt_project.yml:
```yml
models:
  my_project:
    marts:
      product:
        tags: ["hourly"]
      finance:
        tags:
          - "daily"
          - "important"
```

In a model:
```sql
{{config(
    materialized = 'table',
    tags = ["hourly"]
)}}
```

In deployment:
```
dbt run -m tags:hourly
```
]
---

## I am getting permission denied errors on relations that dbt creates
--

```txt
-- snowflake
SQL compilation error: Object 'ANALYTICS.DBT_CLAIRE.MY_TABLE' does not exist or not authorized.

-- redshift
ERROR:  permission denied for relation my_table

```
???


--

### Technique: version-controlled grant statements

???
We want to ensure that as soon as dbt creates a relation, my BI tool can `select` from it
There's a few ways to do this, and it's highly warehouse dependent

--
```sql
grant usage on schema dbt_claire to role reporter;
grant select on dbt_claire.customers to role reporter;
```
???
Essentially we want to run a statement _like_ this

---
# Tool 1: Hooks
* Snippets of SQL that you can run:
    * `on-run-start`: at the beginning of a `dbt run` or `dbt seed`
    * `on-run-end`: at the end of a run
    * `pre-hook`: before a model runs
    * `post-hook`: after a model runs

???
Side step into hooks very quickly to look at it as a way to do this
---
# Tool 1: Hooks
* Configured from the `dbt_project.yml` file or in a model

```yml
# dbt_project.yml
on-run-end:
  - "grant usage on {{ target.schema }} to role reporter"

models:
  - post-hook: "grant select on {{ this }} to role reporter"
```

```sql
-- models/my_model
{{ config(post_hook="grant select on " ~ this ~ "to role reporter")}}
select ...
```

.caption[
  [Learn more about hooks](www.getdbt.com/docs/hooks)
]

---
# Tool 2: Operations
A _macro_ that you can run using the `run-operation` command

```sql
-- macros/grant_select.sql
{% macro grant_select() %}
{% set grant_sql %}
grant usage on schema {{ target.schema }} to role reporter;
grant select on dbt_claire.customers to role reporter;
{% endset %}
{% do run_query(grant_sql) %}
{% endmacro %}

```

Command line:
```bash
$ dbt run-operation grant_select
```

.caption[
  [Learn more about operations](www.getdbt.com/docs/using-operations)
]

???
Note that you have to run the query
These won't work, but here's how we put it together
---
# Implementation details
Consider:
* Are you using multiple schemas? You'll need to grant the `schemas` variable
* Can you leverage default/future grants?
* Can you abstract this logic into a macro instead of hooks?
* Redshift: Are you granting to a single user, or an entire group?


[Our preferred approach](https://discourse.getdbt.com/t/the-exact-grant-statements-we-use-in-a-dbt-project/430)

---

# Other use cases for hooks & operations
* Creating UDFs at the start of a run ([Discourse](https://discourse.getdbt.com/t/using-dbt-to-manage-user-defined-functions/18))
* Performing warehouse maintenance on Redshift ([package](https://github.com/fishtown-analytics/redshift))
* Staging external tables ([package](https://github.com/fishtown-analytics/redshift))


---
# I need to create a table of all days
???
Useful for "date spining"
--

## Technique: dbt-utils.date_spine
---
# Anatomy of a date spine

`models/utils/util_days.sql`
```sql
-- models/utils/util_days.sql
{{ dbt_utils.date_spine(
    datepart="day",
    start_date="to_date('2016/01/01', 'mm/dd/yyyy')",
    end_date="dateadd(week, 1, current_date)"
) }}

```
???
Also note that you need the utils package installed, and to `dbt deps`

--
| date_day   |
|------------|
| 2016-01-01 |
| 2016-01-02 |
| ...        |

--

.caption[
  [Discourse post](https://discourse.getdbt.com/t/finding-active-days-for-a-subscription-user-account-date-spining/265)
]

---
# My runs take too long when I'm developing
--

## Technique: Limit data in dev
???
* Especially useful for event-style data
---
# `{{ target }}`
* A dbt-specific Jinja variable containing information about your current target
    * `{{ target.name }}`
    * `{{ target.schema }}`
    * `{{ target.type }}`
    * `{{ target.user }}`
    * etc

[Learn more](https://docs.getdbt.com/docs/target)

???
* These docs are dbt docs, not Jinja docs

---
# Anatomy of limiting data in dev

```sql
with pages as (

    select * from {{ source('snowplow', 'pages') }}

    {% if target.name == 'dev' %}
    where event_date >= dateadd('day', -3, current_date)
    {% endif %}

),
...
```

???
Tradeoffs:
* This doesn't work so well for relational data â€” you can lose relational integrity
* Also how do you then test that your dashboards work as expected?

---
# I want to add a threshold to the uniqueness test
--

```yml
version: 2

models:
  - name: dim_customers
    columns:
      - name: customer_id
        tests:
          - unique:
              error_threshold: 10
```
???


We already know that:
- dbt has its own project
- tests are just macros prefixed with `test_`
- your own macro will take precedence over dbt's macros

--
## Technique: custom schema test

---
# Anatomy of a custom schema test
1. View the [source code](https://github.com/fishtown-analytics/dbt/tree/dev/0.15.2/core/dbt/include/global_project) for an existing test
2. Copy it into your project!
3. Adjust as required

???
- This is a thing you won't do very often, so it's okay to not know this off by heart!

---
## Example
.denser-text[
```sql
-- macros/test_unique.sql
{% macro test_unique(model) %}

{% set column_name = kwargs.get('column_name', kwargs.get('arg')) %}
{% set error_threshold = kwargs.get('error_threshold', 0) %}

with validation_errors as (
    select
        {{ column_name }}
    from {{ model }}
    where {{ column_name }} is not null
    group by {{ column_name }}
    having count(*) > 1
),

aggregated as (
    select
        count(*) as n_errors
    from validation_errors
    group by 1
),

select
    case
      when n_errors <= {{ error_threshold }} then 0
      else n_errors
    end as result
from aggregated

{% endmacro %}
```
]
---
# Example

```yml
version: 2

models:
  - name: dim_customers
    columns:
      - name: customer_id
        tests:
          - unique:
              error_threshold: 10
```
???
* We've refactored the SQL to make it a bit more readable
* We have a default threshold of 0 so we don't need to add it to every test

---
# Other use cases for custom schema tests
* Build new schema tests, e.g. [mutually exclusive ranges](https://github.com/fishtown-analytics/dbt-utils#mutually_exclusive_ranges-source)
* [Only test in a certain environment](https://discourse.getdbt.com/t/conditionally-running-dbt-tests-only-running-dbt-tests-in-production/322)

.caption[
  [Learn more](https://docs.getdbt.com/docs/custom-schema-tests) / [More examples](https://github.com/fishtown-analytics/dbt-utils#schema-tests)
]

---
# I need to write an extremely bespoke test
???
* Truly a one-off data test

--

## Technique: data tests
---
# Anatomy of a data test
`tests/refund_cannot_exceed_amount.sql`
```sql
-- refunds have a negative amount, so the total amount should always be > 0
select
    order_id,
    sum(amount) as total_amount

from {{ ref('fct_payments' )}}

group by 1

having total_amount < 0
```
Run with `dbt test` (or `dbt test --data`)
.caption[
  N.B. Data tests return **rows**, whereas schema tests return a **number**, `count(*)`. [Learn more](https://docs.getdbt.com/docs/testing)
]

---

# I have some SQL that I use in a dashboard that I want to version control

--
## Technique: analyses
---
# Anatomy of an analysis

`analysis/mrr_dashboard.sql`

```sql
select
    date_month,

    sum(is_active::integer) as customers,
    sum(mrr) as mrr

from {{ ref('fct_mrr') }}

group by 1
```
.caption[
[Learn more](https://docs.getdbt.com/docs/analyses)
]
---
# (SQL) Techniques of the trade

???


---
# I need a "single customer view"
--

But my customers have different IDs in:
* my app
* Zendesk
* Shopify
* Segment

???
How do you answer how many people did X and then went on to do Y?

--

## Technique: user stitching

---
# Anatomy of user stitching
Create master "customers" table with one record per (best-guess) of a human being, with a unique customer identifier


???
* Pro tip: on Snowflake and BQ, you can list the mapped IDs in an array!
* Start simple, make it more complex as you go


---
# What we have:
.denser-text[
`snowplow.page_views`:

| pageview_id | date         | user_id | anonymous_id |
|-------------|--------------|---------|--------------|
| 13jkbse2    | 01/10/2019   |         | 123456       |
| 372jdka     | 01/29/2019   |         | 123456       |
| 9nd2353     | 01/31/2019   |         | 123456       |
| 84jkr939    | 02/03/2019   |         | 123456       |
| jsd8j2kw    | 03/19/2019   | 1       | 123456       |

`jaffle_shop.customers`:

| customer_id  | email                       |
|--------------|-----------------------------|
| 1            | alice@fishtownanalytics.com |

`stripe.customers`:

| customer_id | email                         |
|-------------|-------------------------------|
| 73          | alice@fishtownanalytics.com   |
]

---

# What we want:
| customer_id | snowplow_anonymous_id | stripe_customer_id | email                       |
|-------------|-----------------------|--------------------|-----------------------------|
| 1           | 123456                | 73                 | alice@fishtownanalytics.com |

???
What happens when you have duplicate users? You'll need to dedupe them! Maybe you'll have mapping pairs:
* "mapping pairs" help you say "If a user has _this_ ID in Zendesk, then they have _this_ ID in our customers table (can be many-to-one)
* Which leads us to...

---
# I have duplicate records
--

## Technique: deduplication

---
# Anatomy of deduplication
* Use `distinct` + window functions for narrow tables

```sql
select distinct
    customer_id,

    -- use the most recent email address
    last_value(email ignore nulls) over (
        partition by customer_id
        order by created_at
        rows between unbounded preceding and unbounded following
    ) as email,

    min(created_at) over (
        partition by customer_id
    ) as first_seen_at

from amazon_orders
```
---
# Anatomy of deduplication
* Use `row_number` + `where` for wide tables

.left-column[
  ```sql
  with users as (
    select
      *,
      row_number() over (
          partition by user_id
          order by created_at desc
      ) = 1 as is_most_recent_record
    from users
  )

  select * from users
  where is_most_recent_record
  ```
]
--
.right-column[
  ```sql
  -- snowflake only!
  select
    *,
    row_number() over (
        partition by user_id
        order by created_at desc
    ) = 1 as is_most_recent_record
  from users
  qualify is_most_recent_record



  ```

]


---
# I have KPIs that my business reports
--

## Technique: metrics table

---
# Anatomy of a metrics table
* One record per day per metrics
* `ref`s one model for each metric and unions them together
* Can also compare to previous year values and targets here

| date_day   | metric_channel | metric_name | metric_type | value | past_7 | past_14 | past_21 | ly_value | ... | target |
|------------|----------------|-------------|-------------|-------|--------|---------|---------|----------|-----|--------|
| 02/06/2019 | marketing      | leads       | count       | 20    | 200    | 300     | 150     | 15       | ... | 15     |
| 02/06/2019 | marketing      | mql         | count       | 7     | 40     | 60      | 50      | 3        | ... | 10     |
| 02/06/2019 | marketing      | sqls        | count       | 2     | 10     | 5       | 20      | 2        | ... | 3      |
| 02/06/2019 | accounting     | revenue     | amount      | 150   | 3000   | 2000    | 1500    | 20       | ... | 100    |

???
They can get tricky to handle in the BI layer:
* You lose the ability to drill into the data when you do this!
* Further aggregation can mess things up, especially for non-additive metrics (e.g. conversion rates)

---

class: subtitle, center, middle
# Fin!

{% endraw %}

{% include options/next_presentation.html %}
