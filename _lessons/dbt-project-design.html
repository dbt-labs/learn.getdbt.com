---
layout: presentation
title: dbt Project Design
estimated_teaching_time: 60
teaching_method: slides
working_session: false
learningObjectives:
  - Understand the trade-offs of various model designs
  - Name the three main interfaces for a dbt project
  - Build intuition for naming conventions for database objects
  - Build intuition for how to use dbt and BI tools together

---

class: title, center, middle
# dbt Project Design

???
""  
dbt Project Design  
""  
---

# So, I have raw data.

.grey[_What do I do with it?_]

???
[Teacher Note]: 
You can read these out or pose these questions to the class - read the audience. 
If they don't seem interactive but you want some interactivity, you can also 
pose these questions to fellow teachers.  
    
""  
In previous sessions we saw that we have raw data in the warehouse.  
  
What should we do it?  
--

Model it!

???
We should model it!  
--

.grey[_How?_]

???
and how are we going to model it?  
--

With dbt!

???
With dbt of course!  
""  
--

.grey[_Cool._]

---

{% include options/focus_slide.html %}

???
""  
In this lesson we're going to focus on:
- Understanding the trade-offs of various model designs
- Naming the three main interfaces for a dbt project
- Building intuition for naming conventions for database objects
- and building intuition for how to use dbt and BI tools together

But first, we need to know what we mean by "model it". It's a broad term!  
""  

---

# "Model it!" actually means:

**1. There is a set of final data artifacts I want to share with stakeholders.**
- "data model design," "data warehouse design," "warehousing"
- It's a careful balance between several trade-offs
- Could fill a book (or several) on this topic. It's a fascinating topic that we'll cover _very_ briefly

???
""  
There's two main things we could mean when we say "model it". The first thing it  
could mean is that there's a set of final artifacts we want to share with stakeholders.  

This is also known as data model design, data warehouse design, or warehousing.  
Choosing how we'll do it is a careful balance of trade-offs which our team will  
need to consider.  

There's numerous methodologies and books on this topic, and while it's fascinating,  
we'll only be covering this subject briefly.  
--

**2. I need a reliable, scalable, intuitive process for generating those final artifacts.**
- With dbt!
- The _real_ interest of this presentation

???
The second thing it could mean is that you need a reliable, scalable, and  
intuitive process for creating those artifacts.  

We can do that with dbt - and that's the real interest of this presentation!  
""  
---

class: subtitle, center, middle
# 1. Final data artifacts

### What is the table or view I'm trying to build?

???
""  
So, let's start with our first point - the final data artifacts. The question  
we're aiming to answer is "what is the table or view I'm trying to build?"  
""  

---
## What is the table or view I'm trying to build?

Our view is that there is no one right answer, but there are good heuristics.

As modern data practitioners, we are inheritors of a decades-long history. It
is our task to determine for the present day which concepts are still relevant.

???
""  
Our viewpoint here is that there is no one right answer, but there are good  
practices we have at our disposal to get there.  
  
As modern data practitioners, it's our duty to determine which practices from  
our decades-long history are still relevant and useful today.  

--

Let's break down the process...

???
So, let's break it down -  
""  
---


# It's 1998. You've got mail. üì¨

```
Dear Analyst,

I would like to know how many widgets we sold in 1997 (last year).

Just widgets in our TV category, that is.

Oh, and if you could slice that number by the brand name of the widget, and
dice it by the country where we sold them, that'd be rad.

Thanks!

Tristan
SVP Marketing Analytics
```

???
""  
Let's go back to 1998. We've got mail:  
[> Read Slide <]  
""  

---

# The desired output:

| Brand | Country | Units_Sold |
|-------|---------|------------|
| GE    | AU      | 120        |
| GE    | UK      | 180        |
| GE    | US      | 300        |
| LG    | AU      | 75         |
| LG    | UK      | 100        |
| LG    | US      | 20         |

???
""  
This is our desired output - You can see that we've got a table with one row per  
brand and country combination, with an aggregate of the units sold.  

--

What is the query you (the analyst) will write to get these results?

???
Let's pause here and close our eyes, and imagine the query we would write in 1998  
to get these results.  
""  
  
[Teacher note:]  
You should actually pause here for a few seconds to allow everyone to think  
about this!  
  
--

This depends on your models!

???
""  
If you arrived at this answer - you're correct! It depends on what our underlying  
data modeling looks like.  
""  

---

### Before: snowflake schema (ca. 2000s)

```sql
SELECT
	B.Brand,
	G.Country,
	SUM(F.Units_Sold)
FROM Fact_Sales F
INNER JOIN Dim_Date D             ON F.Date_Id = D.Id
INNER JOIN Dim_Store S            ON F.Store_Id = S.Id
INNER JOIN Dim_Geography G        ON S.Geography_Id = G.Id
INNER JOIN Dim_Product P          ON F.Product_Id = P.Id
INNER JOIN Dim_Brand B            ON P.Brand_Id = B.Id
INNER JOIN Dim_Product_Category C ON P.Product_Category_Id = C.Id
WHERE
	D.Year = 1997 AND
	C.Product_Category = 'tv'
GROUP BY
	B.Brand,
	G.Country
```

???
""  
In 1998, it would have been typical to see a query like this, written on an  
underlying database which uses a snowflake or star schema. We have a central  
fact table, which we're then joining out to dimensions to get the information we  
need to answer Tristan's question.  
""  

---

### Before: snowflake schema (ca. 2000s)

<img src="/ui/img/lessons/data-warehouses/fact-sales-erd.png" class="img-center">

???
""  
And here's what that schema looks like - again, you can see we have a central  
fact table, and in order to get information like country or brand, we actually  
need to join in other tables in between to get there.  
""  

---

## Related data modeling practices:
- Kimball
- Inmon
- DataVault

???
""  
You might have heard about related data modeling practices such as Kimball,  
Inmon, and DataVault.  
""  

---

### Today: denormalized model

```sql
select
	brand,
	country,
	sum(units_sold) as total

from analytics.fct_sales
where date_trunc('year', sale_date) = '1997-01-01'
  and product_category = 'tv'

group by 1, 2
```

???
""  
Our query today would look something like this. There's a lot less going on  
here - notably there are no joins. Why is that? It's because we're working  
with denormalized data, meaning that all of the relevant information is in the  
same model.  
  
You'll also notice that instead of joining to a date table, we're using  
a database function to date_trunc our sales date.  
""  

---

### Today: denormalized model

`fct_sales`: one record per sale to a customer

.denser-text[

| sale_id | sold_at    | units_sold | store_id | store_number | state_province | country | ean_code      | product_name | brand_name   | product_category_name |
|---------|------------|------------|----------|--------------|----------------|---------|---------------|--------------|--------------|-----------------------|
| 1       | 1997-01-01 | 1          | 1234     | 12           | PA             | US      | 901234-123457 | Widget       | Widgetastic  | tv                    |
| 2       | 1997-01-01 | 2          | 1234     | 12           | MA             | US      | 901234-123457 | Widget       | Widgetastic  | tv                    |
| 3       | 1997-01-01 | 1          | 1234     | 12           | NSW            | AU      | 901234-123457 | Widget       | Widgetastic  | tv                    |

]

???
""  
Here's what the underlying data looks like for our denormalized model.  
You can see that important data such as brand, country, units_sold, year, and  
product category are all located in this model.  
""  

---

### Today: denormalized model (2020)

<img src="/ui/img/lessons/data-warehouses/fact-sales-dag.png" class="img-center">

???
""  
And here's the underlying flow of our data. You can see that we still have  
all of the same underlying tables, but instead of joining all of it at once,  
we've modularized it into various flows which finally come together to  
give us a table of all sales. We'll end up querying from this to get Tristan's  
answer via filters and aggregations.  
""  

---
class: middle
## Sidebar

Should that final "roll up" query live in dbt?

(We're going to come back to this discussion!)

???
""  
but before we continue, everyone should think about that final rollup query with  
our filters and aggregations - should this live in dbt?  
  
Think about it - we'll come back to this later!  
""  

---

### Before: snowflake schema (ca. 2000s)

```sql
SELECT
	B.Brand,
	G.Country,
	SUM(F.Units_Sold)
FROM Fact_Sales F
INNER JOIN Dim_Date D             ON F.Date_Id = D.Id
INNER JOIN Dim_Store S            ON F.Store_Id = S.Id
INNER JOIN Dim_Geography G        ON S.Geography_Id = G.Id
INNER JOIN Dim_Product P          ON F.Product_Id = P.Id
INNER JOIN Dim_Brand B            ON P.Brand_Id = B.Id
INNER JOIN Dim_Product_Category C ON P.Product_Category_Id = C.Id
WHERE
	D.Year = 1997 AND
	C.Product_Category = 'tv'
GROUP BY
	B.Brand,
	G.Country
```

???
""  
So, we've got our query from 1998 -  
""  

---

### Today: denormalized model

```sql
select
	brand,
	country,
	sum(units_sold) as total

from analytics.fct_sales
where date_trunc('year', sale_date) = '1997-01-01'
  and product_category = 'tv'

group by 1, 2
```

???
""  
and we've got our query from today -  
""  

---


# What has changed?

???
""  
What's changed? Why didn't we just denormalize in 1998 or use date_trunc  
instead of having dedicated date tables?  

--

Some design decisions of the past were driven by technology:
- Storage was expensive ‚Üí don't duplicate data
- Joins were cheap ‚Üí join at report-time
- SQL functions were expensive ‚Üí don't `date_trunc`, create a table that has already done it.
- Updating data was expensive ‚Üí design models so that changes are isolated to one table
- Data modeling was a different skill set to analysis ‚Üí project plan the whole thing!

???
It's because the decisions we made in the past and the decisions we make today  
are driven by technology, namely in the way that row-oriented and  
column-oriented databases store and process data.  
  
For row-oriented databases:
- Storage was expensive, so you wouldn't want to duplicate data
- Joins were cheap, so it was fine join at report-time
- SQL functions were expensive, so instead of using `date_trunc`, it was better 
to create a table that has already done it.
- Updating data was expensive, so it was better to design models so that changes
were isolated to one table
- and data modeling was a different skill set to analysis - the entire schema
would go through project planning before ever reaching analysis!
  
Because we use columnar-store for our analytics processing today, it allows us  
normalize or denormalize as we'd like, comfortably use SQL functions, and start  
closing the gap between modeling and analysis.  
""  

---

# But what has stayed the same?

???
""  
But, what's stayed the same?  

--

- A "final" model should represent a _business_ process or entity.

???
A final model should always represent a business process or entity. For example,  
orders, customers, or suppliers.  

--

**Example 1:**

Let's say you change payment providers ‚Äî to your finance team, these are still
a "payment".

It might be useful to track whether it was processed via `stripe`
or `braintree` (i.e. include a `payments_provider` column), but ultimately, the
`payments` table should include both in a standardized form.

???
Let's say we had a legacy payment system, `braintree`, and a new payment  
system, `stripe`. Our stakeholders only know these things as `payments`, and  
it should be represented as such.  
  
It might be useful to add a column with information on how the payment was  
processed (i.e, 'braintree' or 'stripe'), but in the end the payments table  
should include all records in a standardized form.  
""  

---

# But what has stayed the same?

- A "final" model should represent a _business_ process or entity.
- Each model should have a clear "grain" ‚Äî "one record represents a ..."

???
""  
Each model should represent a clear grain - you should be able to say "each  
record represents a ...".  

--

**Example 2:**

Let's say you're Rideshare Inc.  Every time someone splits a trip,
the `trips` table has three entries: one parent trip, and two child trips.

| trip_id | parent_id | .. |
|---------|-----------|----|
| 1       | NULL      | .. |
| 2       | 1         | .. |
| 3       | 1         | .. |
| 4       | NULL      | .. |

How many trips occurred?

???
For example, say we have a rideshare company and each time someone splits a trip,  
the trips table had three entries: one for the parent trip, and two child trips.  

How many trips occured?  

We could say that:
- two trips occured, we're only counting trips without a parent trip as a trip.
- three trips occurred, because we're counting those without a parent trip as 
a trip, and the split trip as one trip.
- or four trips occurred because we're counting each trip id

--
 Well what's a trip?

???
Well, what's a trip...?  
In this example, we had to explain our trips by saying: "each record represents  
a trip, but ...".  
""  

---
# But what has stayed the same?

- A "final" model should represent a _business_ process or entity.
- Each model should have a clear "grain" ‚Äî "one record represents a ..."
- Naming is important. In particular, consistency is _extremely_ valuable

???
""  
Naming always has been, and always will be, important. It's extemely valuable  
so your team can understand the inputs and outputs of your analytics ecosystem.  

--
- Consider the final use case when designing your final artifacts

???
And finally, we always want to keep in mind what the final use case is:
- Who will need to our final artifacts?
- Which questions do we need to answer related to these data sets?
- How well will the tools consuming our data products handle how we propose to model it?

""  

---

### OK, but _really_ what is the final artifact that I should build?

Should I denormalize everything into one wide table? Should I use Kimball design patterns?

???
""  
We still haven't answered the question of what the final artifacts we should  
be building are. Should they be wide denormalized tables? Should we use a Kimball  
design pattern?  

--

The answer is: it depends!

- What's your BI tool? Does it support joins in the BI layer (e.g. Looker), or
does it prefer flat datasets (e.g. Tableau).
- How big is your data? How does data get updated? e.g. do you have millions of
orders that don't update after they are complete, but product names that update
without warning? (Maybe join it in the BI tool)
- What data models are your stakeholders used to? e.g. Do you work at an enterprise
company that is used to working with DataVault models?

???
Our answer: it depends! For example:
- If your BI tool is great with joins such as Looker, you can handle being a little
more normalized. However, if it prefers flat data sets like Tableau, you might
choose to denormalize.
- If you have data sets that update in unpredictable ways, you might fair 
better joining this in the BI tool so you can keep the information isolated 
to understand what you're looking at.
- If your company and team is used to working with a specific methodology, then 
sometimes it's just easier to go with the flow!

--

And dbt lets you choose! It's all just SQL üòä

???
In the end, it's all just SQL. You can choose what makes the most sense for your  
own business practices and tooling!  

""  

---

## So let's say you have an idea of what you want to build

<img src="/ui/img{{page.id}}/dag-part-1.png" style="width: 80%;" class="img-center">


Once you have an intuition of what you want to build, how do you do that in dbt?

???
""  
So, let's say we _do_ have an idea of what we want to build. How do we do that  
with dbt?  
""  

---
class: subtitle, center, middle

# 2. The process is the product

### (and that's more important)

???
""  
This is where we'll talk about the process - how we'll create our artifacts in a  
scalable, reliable, and intuitive way.  
""  

---

class: subtitle, middle

_The natural state of the universe is chaos: entropy tends to increase in closed
systems, and there‚Äôs really nothing that we can do about that. So too is the
nature of your dbt project: unless action is taken to maintain order._

???
""  
Let's start with this quote:  
The natural state of the universe is chaos: entropy tends to increase in closed  
systems, and there‚Äôs really nothing that we can do about that. So too is the  
nature of your dbt project: unless action is taken to maintain order.  
""  

---
# What we really mean to say:

No matter what the final artifacts you're building are, be effortful in how you
organize them.

These artifacts show up in three places:
1. The dbt DAG
2. The repo
3. The warehouse

These are how people at your organization will conceptualize, interact with,
and benefit from data‚Äîyour product‚Äîmore than any one table or dashboard.

???
""  
What we really mean by that is that no matter what you're building, be effortful  
in how you organize your artifacts.  

The three main places where people at your organization will interact with your  
products are the DAG, the repository, and the warehouse.  
""  

---

# Interface #1: The dbt DAG


"Where are those sales numbers coming from?" &rarr; _Which data sources, processes, and assumptions do our sales calculations depend on?_

The DAG tells the story:

<img src="/ui/img{{page.id}}/dag-simple.png" style="width: 80%;" class="img-center">

<h3 style="text-align: center;"><span style="color: green;">Source</span>
&rarr;
<span style="color: blue;">Staging</span>
&rarr;
<span style="color: purple;">Dimensions/Facts</span></h3>

???
""  
Let's look at the DAG:
The DAG reflects how data flows from source to end product through your project.  

If you've ever been asked "where are those sales numbers coming from?", what  
you're really being asked about how the end result is:
- consuming sources of data
- being processed
- and being calculated (what are the assumptions?)

Our DAG tells the story -  
You can see here that our sales model combines order and payment information from  
jaffle_shop and stripe, cleans the data, and finally comes together in fct_sales.  
This makes it easy for us to make informed decisions and understand where to  
look for more information.  
""  

---

# Interface #1: The dbt DAG

For your final artifacts, consider which sources will power this project:

<img src="/ui/img{{page.id}}/dag-part-2.png" style="width: 70%;" class="img-center">

(More on this later)

???
""  
In order to plan out our flow of data, let's whiteboard our example.  
We'll start by considering the the sources that power our end result, fct_sales.   
The data sources we need are orders, products, customers, and stores.  

---

# Interface #1: The dbt DAG

Then add staging models: "The shape you wish your data came in"

<img src="/ui/img{{page.id}}/dag-part-3.png" style="width: 70%;" class="img-center">

???
Once we have our sources, we create our first layer of models which cleans and  
standardize our raw data so that it's easier to work with downstream.  
""  

---

# Interface #1: The dbt DAG
### Model prefixes
.dense-text[
.left-column-33[
### stg_
- 1:1 with raw data tables
- **Source**-conformed
- Renaming, type casting, coalescing + nullif‚Äôing
]

.right-column-66[
.left-column[
### dim_
- ‚ÄúNouns‚Äù
- Core **business** entities
- Customers, products, apartments, providers, employees, ...
- Some "slowly-changing" attributes (e.g. address, email)
]

.right-column[
### fct_
- ‚ÄúVerbs‚Äù
- Core **business** processes
- Orders, cases, plays, listings, comments, ...
- Often built on top of an immutable event stream
]]]

???
""  
So far, you've seen the following prefixes mentioned. It's great to have a naming  
convention for your models to flag developers on their contents or purposes.  
At dbt Labs, here's what our prefixes mean:
- stg_ means that we're cleaning and standardizing raw data: 
  - We keep them 1:1 with our source tables, meaning that one model is cleaning  
    one table - no joins.
  - We also don't aggregate here, we keep it relatively source conformed - meaning  
    that the grain is kept the same aside from any light filtering, such as filtering  
    out deleted records.
  - The types of SQL you'll see here are things like:
        - renaming
        - casting
        - coalescing
        - null-if'ing 
        - case when'ing
        - row_number'ing
    
- dim_ and fct_ are final artifacts that will be used in analysis. Typically  
these are surfaced to stakeholders or outside tooling (such as a BI tool).  
  - dim_ stands for dimension and usually describes an entity or object, 
    such as customers, products, apartments, or providers.
  - dimensions usually update by overwriting data.
  - fct_ describes something that's happening, i.e, an action or verb such as
    orders, cases, plays, or listings, or comments.
    - they usually append data rather than overwrite data, because the data is
      logged as one row per action taken. 

[Teacher note:]
If anyone asks why we say "typically surfaced to outside tooling", you can give
the example of typical modeling seen with Tableau. Tableau does better when you
pre-aggregate data sets before piping them in to reports. For this reason, you 
might create a "reports" folder with report_ prefixed models which do your joins
and aggregations on the major concepts (the dims and fcts). 
- 
""  
---

<h3 style="text-align: center;"><span style="color: green;">source</span>
&rarr;
<span style="color: lightblue; font-size: 60%;">[base]</span>
&rarr;
<span style="color: blue;">stg</span>
&rarr;
<span style="color: grey; font-size: 60%;">[intermediate, lookup, utils]</span>
&rarr;
<span style="color: purple;">{fct</span>
+
<span style="color: navy;">dim}</span></h3>

<img src="/ui/img{{page.id}}/dag-grouped.png" style="width: 100%;">

???
Instructor: draw circles/boxes around the different "model groups" (sources,
staging, intermediate, lookups, final fact/dim). Comment on how they can
be grouped together in all three interfaces.

---
# Interface #1: The dbt DAG

Make sure that for each model, you ask:
- What does one record represent? Let's document this!
- What assumptions about our data are we making? We should test those!
- Am I using naming patterns? You should!

---

# Interface #2: The repo

.left-column[
- **Within files:** adhere to a style guide
- **Between files:** better file organization = easier onboarding + better collaboration
]

.right-column[
.dense-text[
```
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ dbt_project.yml
‚îî‚îÄ‚îÄ models
    ‚îú‚îÄ‚îÄ marts
        ‚îî‚îÄ‚îÄ core
            ‚îú‚îÄ‚îÄ core.yml
            ‚îî‚îÄ‚îÄ dim_product.sql
            ‚îú‚îÄ‚îÄ dim_store.sql
            ‚îî‚îÄ‚îÄ fact_sales.sql
    ‚îî‚îÄ‚îÄ staging
        ‚îî‚îÄ‚îÄ postgres
        ‚îî‚îÄ‚îÄ shopify
            ‚îú‚îÄ‚îÄ shopify.yml
            ‚îú‚îÄ‚îÄ stg_shopify_customers.sql
            ‚îú‚îÄ‚îÄ stg_shopify_order_items.sql
            ‚îú‚îÄ‚îÄ stg_shopify_orders.sql
            ‚îî‚îÄ‚îÄ stg_shopify_products.sql
```
]
]

---

# Interface #3: The Warehouse

.left-column-66[
- **Nomenclature.** How do we name our views + tables so they are consistent, easy to
find, and easy to infer contents?
- How do we organize our output objects? Same schema or different?
- What about the intermediate stuff we _don‚Äôt_ want to show?
- Who is your end user? Is it a person writing SQL, or a BI tool pointed to a production schema?
]

.right-column-33[
<img src="/ui/img{{page.id}}/mode-sidebar.png" style="width: 60%;" class="img-center">
]

---

class: subtitle, center, middle
# The way we roll

---

<img src="/ui/img{{page.id}}/discourse-stats.png" class="img-center">
&nbsp;
<img src="/ui/img{{page.id}}/discourse-intro.png" class="img-center">

.center[[Read it!](https://discourse.getdbt.com/t/how-we-structure-our-dbt-projects/355)]

---

class: subtitle

# Checkpoint:

- What are some helpful actions to do in the staging layer?  Why?
--

- For those who have worked with dim/fct tables, how do you think about the differences?
- Note: The difference can be a bit blurry if you are learning them for the first time.

--

## Questions?

---

class: subtitle, center, middle
# dbt + BI

## What BI tools do you use?

---

class: center
# What BI tools do you use?
<img src="/ui/img{{page.id}}/bi-logos-3.png" style="width: 40%;" class="img-center">
---

# Which part of this should be in the dbt versus the BI tool?

<img src="/ui/img{{page.id}}/dag-part-3.png" style="width: 70%;" class="img-center">

---

# Which part of this should be in the dbt versus the BI tool?

For most orgs, the final rollup query should be in BI tool ‚Äî allows for "drill down" functionality / further exploration.

--

What if?
- This is a KPI your company reports on regularly?
--
 Maybe you want it in a dbt model as well.

--
- You're building a prototype model or field?
--
 Potentially build it in the BI tool first then "promote" it.
---


# What should you use your BI tool for?

---
## The things they've always been good at!

.left-column-33[
- User Interface
    - Point-and-click
    - Self-service filters, drill-downs
    - Accessible to users who may be less technical
]
.right-column-66[
.left-column[
- Viz!
    - Out-of-the-box
	- Color palettes ("my brand!")
    - Frameworks/libraries
]
.right-column[
- Nice-to-haves
    - Dynamic user input
    - Scheduling
	- Alerting
	- PDF‚Äôing
	- CSV‚Äôing
]
]

---

## Not the things they're less good at

- Multilayered data transformation with complex ordered dependencies
- Enforcing a single sources of truth
    - Defining, documenting, centralizing business logic
- Maintaining separate data environments
    - Testing
    - Version control
- Running a lot non-performant SQL concurrently
- Managing user access to sensitive data

---

# What goes where?

.left-column[
## dbt layer
- Critical business logic
- Best agreed-on version of a model
- Complex SQL
- No wasted code! Everything is multipurpose*
- Anything worth version control
]

.right-column[
## BI layer
- Aggregations, calculated on the fly
- Joins qualified by user input
- Ad hoc queries and proto-models
- Select-star SQL that feeds R, Python, JS (custom viz)
- Nothing you can't take with you
]

???
dbt models are like kitchen tools. The only single-purpose kitchen appliance
you should ever have is a rice cooker.

BI assets are like recipes. It doesn't hurt as much to have similar ones, but
when it comes time to make dinner, your guests might become confused :)

---

class: subtitle

# Checkpoint:

- What are the three interfaces into your dbt project?
--

- What are some key distinctions between the dbt layer and BI layer that you want to keep in mind?
--

- What naming conventions is your team already using?

--

## Questions?

---

class: subtitle

# Knowledge check

When creating a dbt project:
- First decide on the final artifact, and then sketch out the process for modeling it.
- Use the three interfaces to organize your models.
- Stage your raw source data before transforming it.
- Every model should have a primary key.
- Decide as an organization how to keep your BI layer thin and flexible.

{% include options/last_slide.html %}

---

class: subtitle, middle, center
# Appendix: Words to the wise
### from one data modeler to another

???
This is a very special section, for anyone in the room who feels like the
presentation thus far has been too _descriptive_ ("what's hard about modeling?") and not
_prescriptive_ enough ("ok great, now _what should I do?_")

---

1. **Nomenclature** is _hard_, and it's worth your time to be consistent. It doesn't
matter if you prefer `fct_` or `fact_`, it's important that you choose one. (What's not hard?
Every model/object name is snake case + case insensitive.)
---
1. .grey[**Nomenclature.**]
2. **Public vs. Private.** Prefixed models with single underscores represent external/usable assets.  Double underscores represent internal/intermediate assets.
---
1. .grey[**Nomenclature.**]
2. .grey[**Public vs. Private.**]
3. **Modular model files:** Individual models should max out at a half-dozen
CTEs / few hundred lines of SQL. The CTEs should be named such that they narrate
the model's transformation. If one model file is getting too long, split it up
into multiple ephemeral models.
---
1. .grey[**Nomenclature.**]
2. .grey[**Public vs. Private.**]
3. .grey[**Modular model files.**]
4. **Macros in moderation:** Model code should seek to be _both_ DRY (Don't
Repeat Yourself) _and_ easily readable. Use Jinja + macros strongly and sparingly.
---
1. .grey[**Nomenclature.**]
2. .grey[**Public vs. Private.**]
3. .grey[**Modular model files.**]
4. .grey[**Macros in moderation.**]
5. **Reusable assets:** Final models (facts + dimensions) should be multipotent,
such that each can power an entire genre of potential queries. Your end user
or BI tool should be able to answer 80% of its most common questions by
querying **one (1)** table. Of the remaining 20%, no regularly running query
should require more than two join "steps." As soon as an ad hoc query is run
often, it should be refactored and modeled for accordingly.
---
1. .grey[**Nomenclature.**]
2. .grey[**Public vs. Private.**]
3. .grey[**Modular model files.**]
4. .grey[**Macros in moderation.**]
5. .grey[**Reusable assets.**]
6. **Documentation, documentation, documentation:** Your data models are your
product; the BI/query layer is your user experience. Having an untested,
undocumented model is worse than having no model at all.
<br>
<br>
_Corollary:_ Uncommented, unvetted, unreadable SQL‚Äîhowever clever
it may be‚Äîis _much_ worse than having no code at all.

---
1. .grey[**Nomenclature.**]
2. .grey[**Public vs. Private.**]
3. .grey[**Modular model files.**]
4. .grey[**Macros in moderation.**]
5. .grey[**Reusable assets.**]
6. .grey[**Documentation, documentation, documentation.**]
7. **Be your own best advocate:** Your dimensional modeling is your colleague's
feature engineering is your stakeholder's reconciliation of business definitions.
Analysts, BI users, business partners, and data scientists all can and should be
beneficiaries of the foundational work you're doing.
<br>
<br>
_Corollary:_ If they expend effort defining the same concepts on their own,
it's not only repeated work, but fertile ground for entropy at your organization.

???
I think about join "steps" as degrees of Kevin Bacon
---
1. .grey[**Nomenclature.**]
2. .grey[**Public vs. Private.**]
3. .grey[**Modular model files.**]
4. .grey[**Macros in moderation.**]
5. .grey[**Reusable assets.**]
6. .grey[**Documentation, documentation, documentation.**]
6. .grey[**Be your own best advocate.**]

# YMMV!
Take it in good health, and with many grains of salt.
