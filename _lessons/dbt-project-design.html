---
layout: presentation
title: dbt Project Design
estimated_teaching_time: 60
teaching_method: slides
working_session: false
learningObjectives:
  - Understand the trade-offs of various model designs
  - Name the three main interfaces for a dbt project
  - Build intuition for naming conventions for database objects
  - Build intuition for how to use dbt and BI tools together

---

class: title, center, middle
# dbt Project Design

???
""  
dbt Project Design  
""  
---

# So, I have raw data.

.grey[_What do I do with it?_]

???
[Teacher Note]: 
You can read these out or pose these questions to the class - read the audience. 
If they don't seem interactive but you want some interactivity, you can also 
pose these questions to fellow teachers.  
    
""  
In previous sessions we saw that we have raw data in the warehouse.  
  
What should we do it?  
--

Model it!

???
We should model it!  
--

.grey[_How?_]

???
and how are we going to model it?  
--

With dbt!

???
With dbt of course!  
""  
--

.grey[_Cool._]

---

{% include options/focus_slide.html %}

???
""  
In this lesson we're going to focus on:
- Understanding the trade-offs of various model designs
- Naming the three main interfaces for a dbt project
- Building intuition for naming conventions for database objects
- and building intuition for how to use dbt and BI tools together

But first, we need to know what we mean by "model it". It's a broad term!  
""  

---

# "Model it!" actually means:

**1. There is a set of final data artifacts I want to share with stakeholders.**
- "data model design," "data warehouse design," "warehousing"
- It's a careful balance between several trade-offs
- Could fill a book (or several) on this topic. It's a fascinating topic that we'll cover _very_ briefly

???
""  
There's two main things we could mean when we say "model it":  

1. The first thing it could mean is that there's a set of final artifacts we 
want to share with stakeholders.  

This is also known as data model design, data warehouse design, or warehousing.  
Choosing how we'll do it is a careful balance of trade-offs which our team will  
need to consider.  

There's numerous methodologies and books on this topic, and while it's fascinating,  
we'll only be covering this subject briefly.  

--

**2. I need a reliable, scalable, intuitive process for generating those final artifacts.**
- With dbt!
- The _real_ interest of this presentation

???
2. The second thing it could mean is that you need a reliable, scalable, and  
intuitive process for creating those artifacts.  

We can do that with dbt - and that's the real interest of this presentation!  
""  
---

class: subtitle, center, middle
# 1. Final data artifacts

### What is the table or view I'm trying to build?

???
""  
So, let's start with our first point - the final data artifacts. The question  
we're aiming to answer is "what is the table or view I'm trying to build?"  
""  

---
## What is the table or view I'm trying to build?

Our view is that there is no one right answer, but there are good heuristics.

As modern data practitioners, we are inheritors of a decades-long history. It
is our task to determine for the present day which concepts are still relevant.

???
""  
Our viewpoint here is that there is no one right answer, but there are good  
practices we have at our disposal to get there.  
  
As modern data practitioners, it's our duty to determine which practices from  
our decades-long history are still relevant and useful today.  

--

Let's break down the process...

???
So, let's break it down -  
""  
---


# It's 1998. You've got mail. 📬

```
Dear Analyst,

I would like to know how many widgets we sold in 1997 (last year).

Just widgets in our TV category, that is.

Oh, and if you could slice that number by the brand name of the widget, and
dice it by the country where we sold them, that'd be rad.

Thanks!

Tristan
SVP Marketing Analytics
```

???
""  
Let's go back to 1998. We've got mail:  
[> Read Slide <]  
""  

---

# The desired output:

| Brand | Country | Units_Sold |
|-------|---------|------------|
| GE    | AU      | 120        |
| GE    | UK      | 180        |
| GE    | US      | 300        |
| LG    | AU      | 75         |
| LG    | UK      | 100        |
| LG    | US      | 20         |

???
""  
This is our desired output - You can see that we've got a table with one row per  
brand and country combination, with an aggregate of the units sold.  

--

What is the query you (the analyst) will write to get these results?

???
Let's pause here and close our eyes, and imagine the query we would write in 1998  
to get these results.  
""  
  
[Teacher note:]  
You should actually pause here for a few seconds to allow everyone to think  
about this!  
  
--

This depends on your models!

???
""  
If you arrived at this answer - you're correct! It depends on what our underlying  
data modeling looks like.  
""  

---

### Before: snowflake schema (ca. 2000s)

```sql
SELECT
	B.Brand,
	G.Country,
	SUM(F.Units_Sold)
FROM Fact_Sales F
INNER JOIN Dim_Date D             ON F.Date_Id = D.Id
INNER JOIN Dim_Store S            ON F.Store_Id = S.Id
INNER JOIN Dim_Geography G        ON S.Geography_Id = G.Id
INNER JOIN Dim_Product P          ON F.Product_Id = P.Id
INNER JOIN Dim_Brand B            ON P.Brand_Id = B.Id
INNER JOIN Dim_Product_Category C ON P.Product_Category_Id = C.Id
WHERE
	D.Year = 1997 AND
	C.Product_Category = 'tv'
GROUP BY
	B.Brand,
	G.Country
```

???
""  
In 1998, it would have been typical to see a query like this, written on an  
underlying database which uses a snowflake or star schema. We have a central  
fact table, which we're then joining out to dimensions to get the information we  
need to answer Tristan's question.  
""  

---

### Before: snowflake schema (ca. 2000s)

<img src="/ui/img/lessons/data-warehouses/fact-sales-erd.png" class="img-center">

???
""  
And here's what that schema looks like - again, you can see we have a central  
fact table, and in order to get information like country or brand, we actually  
need to join in other tables in between to get there.  
""  

---

## Related data modeling practices:
- Kimball
- Inmon
- DataVault

???
""  
You might have heard about related data modeling practices such as Kimball,  
Inmon, and DataVault.  
""  

---

### Today: denormalized model

```sql
select
	brand,
	country,
	sum(units_sold) as total

from analytics.fct_sales
where date_trunc('year', sale_date) = '1997-01-01'
  and product_category = 'tv'

group by 1, 2
```

???
""  
Our query today would look something like this. There's a lot less going on  
here - notably there are no joins. Why is that? It's because we're working  
with denormalized data, meaning that all of the relevant information is in the  
same model.  
  
You'll also notice that instead of joining to a date table, we're using  
a database function to date_trunc our sales date.  
""  

---

### Today: denormalized model

`fct_sales`: one record per sale to a customer

.denser-text[

| sale_id | sold_at    | units_sold | store_id | store_number | state_province | country | ean_code      | product_name | brand_name   | product_category_name |
|---------|------------|------------|----------|--------------|----------------|---------|---------------|--------------|--------------|-----------------------|
| 1       | 1997-01-01 | 1          | 1234     | 12           | PA             | US      | 901234-123457 | Widget       | Widgetastic  | tv                    |
| 2       | 1997-01-01 | 2          | 1234     | 12           | MA             | US      | 901234-123457 | Widget       | Widgetastic  | tv                    |
| 3       | 1997-01-01 | 1          | 1234     | 12           | NSW            | AU      | 901234-123457 | Widget       | Widgetastic  | tv                    |

]

???
""  
Here's what the underlying data looks like for our denormalized model.  
You can see that important data such as brand, country, units_sold, year, and  
product category are all located in this model.  
""  

---

### Today: denormalized model (2020)

<img src="/ui/img/lessons/data-warehouses/fact-sales-dag.png" class="img-center">

???
""  
And here's the underlying flow of our data. You can see that we still have  
all of the same underlying tables, but instead of joining all of it at once,  
we've modularized it into various flows which finally come together to  
give us a table of all sales. We'll end up querying from this to get Tristan's  
answer via filters and aggregations.  
""  

---
class: middle
## Sidebar

Should that final "roll up" query live in dbt?

(We're going to come back to this discussion!)

???
""  
but before we continue, everyone should think about that final rollup query with  
our filters and aggregations - should this live in dbt?  
  
Think about it - we'll come back to this later!  
""  

---

### Before: snowflake schema (ca. 2000s)

```sql
SELECT
	B.Brand,
	G.Country,
	SUM(F.Units_Sold)
FROM Fact_Sales F
INNER JOIN Dim_Date D             ON F.Date_Id = D.Id
INNER JOIN Dim_Store S            ON F.Store_Id = S.Id
INNER JOIN Dim_Geography G        ON S.Geography_Id = G.Id
INNER JOIN Dim_Product P          ON F.Product_Id = P.Id
INNER JOIN Dim_Brand B            ON P.Brand_Id = B.Id
INNER JOIN Dim_Product_Category C ON P.Product_Category_Id = C.Id
WHERE
	D.Year = 1997 AND
	C.Product_Category = 'tv'
GROUP BY
	B.Brand,
	G.Country
```

???
""  
So, we've got our query from 1998 -  
""  

---

### Today: denormalized model

```sql
select
	brand,
	country,
	sum(units_sold) as total

from analytics.fct_sales
where date_trunc('year', sale_date) = '1997-01-01'
  and product_category = 'tv'

group by 1, 2
```

???
""  
and we've got our query from today -  
""  

---


# What has changed?

???
""  
What's changed? Why didn't we just denormalize in 1998 or use date_trunc  
instead of having dedicated date tables?  

--

Some design decisions of the past were driven by technology:
- Storage was expensive → don't duplicate data
- Joins were cheap → join at report-time
- SQL functions were expensive → don't `date_trunc`, create a table that has already done it.
- Updating data was expensive → design models so that changes are isolated to one table
- Data modeling was a different skill set to analysis → project plan the whole thing!

???
It's because the decisions we made in the past and the decisions we make today  
are driven by technology, namely in the way that row-oriented and  
column-oriented databases store and process data.  
  
For row-oriented databases:
- Storage was expensive, so you wouldn't want to duplicate data
- Joins were cheap, so it was fine join at report-time
- SQL functions were expensive, so instead of using `date_trunc`, it was better 
to create a table that has already done it.
- Updating data was expensive, so it was better to design models so that changes
were isolated to one table
- and data modeling was a different skill set to analysis - the entire schema
would go through project planning before ever reaching analysis!
  
Because we use columnar-store for our analytics processing today, it allows us  
normalize or denormalize as we'd like, comfortably use SQL functions, and start  
closing the gap between modeling and analysis.  
""  

---

# But what has stayed the same?

???
""  
But, what's stayed the same?  

--

- A "final" model should represent a _business_ process or entity.

???
A final model should always represent a business process or entity. For example,  
orders, customers, or suppliers.  

--

**Example 1:**

Let's say you change payment providers — to your finance team, these are still
a "payment".

It might be useful to track whether it was processed via `stripe`
or `braintree` (i.e. include a `payments_provider` column), but ultimately, the
`payments` table should include both in a standardized form.

???
Let's say we had a legacy payment system, `braintree`, and a new payment  
system, `stripe`. Our stakeholders only know these things as `payments`, and  
it should be represented as such.  
  
It might be useful to add a column with information on how the payment was  
processed (i.e, 'braintree' or 'stripe'), but in the end the payments table  
should include all records in a standardized form.  
""  

---

# But what has stayed the same?

- A "final" model should represent a _business_ process or entity.
- Each model should have a clear "grain" — "one record represents a ..."

???
""  
Each model should represent a clear grain - you should be able to say "each  
record represents a ...".  

--

**Example 2:**

Let's say you're Rideshare Inc.  Every time someone splits a trip,
the `trips` table has three entries: one parent trip, and two child trips.

| trip_id | parent_id | .. |
|---------|-----------|----|
| 1       | NULL      | .. |
| 2       | 1         | .. |
| 3       | 1         | .. |
| 4       | NULL      | .. |

How many trips occurred?

???
For example, say we have a rideshare company and each time someone splits a trip,  
the trips table had three entries: one for the parent trip, and two child trips.  

How many trips occured?  

We could say that:
- two trips occured, we're only counting trips without a parent trip as a trip.
- three trips occurred, because we're counting those without a parent trip as 
a trip, and the split trip as one trip.
- or four trips occurred because we're counting each trip id

--
 Well what's a trip?

???
Well, what's a trip...?  
In this example, we had to explain our trips by saying: "each record represents  
a trip, but ...".  
""  

---
# But what has stayed the same?

- A "final" model should represent a _business_ process or entity.
- Each model should have a clear "grain" — "one record represents a ..."
- Naming is important. In particular, consistency is _extremely_ valuable

???
""  
Naming always has been, and always will be, important. It's extemely valuable  
so your team can understand the inputs and outputs of your analytics ecosystem.  

--
- Consider the final use case when designing your final artifacts

???
And finally, we always want to keep in mind what the final use case is by asking  
questions like:
- Who will need to use our final artifacts?
- Which questions do we need to answer related to these data sets?
- How well will the tools consuming our data products handle how we propose to model it?

""  

---

### OK, but _really_ what is the final artifact that I should build?

Should I denormalize everything into one wide table? Should I use Kimball design patterns?

???
""  
Okay, but we still haven't answered the question about what the final artifacts  
should be. Should they be wide denormalized tables? Should we use a Kimball  
design pattern?  

--

The answer is: it depends!

- What's your BI tool? Does it support joins in the BI layer (e.g. Looker), or
does it prefer flat datasets (e.g. Tableau).
- How big is your data? How does data get updated? e.g. do you have millions of
orders that don't update after they are complete, but product names that update
without warning? (Maybe join it in the BI tool)
- What data models are your stakeholders used to? e.g. Do you work at an enterprise
company that is used to working with DataVault models?

???
Our answer: it depends! For example:
- If your BI tool is great with joins such as Looker, you can handle being a little
more normalized. However, if it prefers flat data sets like Tableau, you might
choose to denormalize.
- or mayber you have data sets that update in unpredictable ways - it might be
better joining these in the BI tool so you can keep the information isolated 
to understand what you're looking at.
- or maybe your team is used to working with a specific methodology - if that's  
the case, sometimes it's just easier to go with the flow!

--

And dbt lets you choose! It's all just SQL 😊

???
In the end, it's all just SQL. You can choose what makes the most sense for your  
own business practices and tooling!  

""  

---

## So let's say you have an idea of what you want to build

<img src="/ui/img{{page.id}}/dag-part-1.png" style="width: 80%;" class="img-center">


Once you have an intuition of what you want to build, how do you do that in dbt?

???
""  
So, let's say we _do_ have an idea of what we want to build. How do we do that  
with dbt?  
""  

---
class: subtitle, center, middle

# 2. The process is the product

### (and that's more important)

???
""  
This is where we'll talk about the process and how we'll create our artifacts in  
a scalable, reliable, and intuitive way.  
""  

---

class: subtitle, middle

_The natural state of the universe is chaos: entropy tends to increase in closed
systems, and there’s really nothing that we can do about that. So too is the
nature of your dbt project: unless action is taken to maintain order._

???
""  
Let's start with this quote:  
The natural state of the universe is chaos: entropy tends to increase in closed  
systems, and there’s really nothing that we can do about that. So too is the  
nature of your dbt project: unless action is taken to maintain order.  
""  

---
# What we really mean to say:

No matter what the final artifacts you're building are, be effortful in how you
organize them.

These artifacts show up in three places:
1. The dbt DAG
2. The repo
3. The warehouse

These are how people at your organization will conceptualize, interact with,
and benefit from data—your product—more than any one table or dashboard.

???
""  
What we really mean by that is that no matter what you're building, be effortful  
in how you organize your artifacts.  

The three main places where people at your organization will interact with your  
products are the DAG, the repository, and the warehouse.  
""  

---

# Interface #1: The dbt DAG


"Where are those sales numbers coming from?" &rarr; _Which data sources, processes, and assumptions do our sales calculations depend on?_

The DAG tells the story:

<img src="/ui/img{{page.id}}/dag-simple.png" style="width: 80%;" class="img-center">

<h3 style="text-align: center;"><span style="color: green;">Source</span>
&rarr;
<span style="color: blue;">Staging</span>
&rarr;
<span style="color: purple;">Dimensions/Facts</span></h3>

???
""  
Let's talk about the first interface - the DAG.  
A DAG (Directed Acyclic Graph) is generated in every dbt project, and reflects  
how data flows from source to end product.  

If you've ever been asked "where are those sales numbers coming from?", you're  
really being asked how the end result is:
- consuming sources of data
- being processed
- and being calculated (what are the assumptions?)

Our DAG tells the story -  
You can see here that our sales model combines order and payment information from  
jaffle_shop and stripe, cleans the data, and finally comes together in fct_sales.  
""  

---

# Interface #1: The dbt DAG

For your final artifacts, consider which sources will power this project:

<img src="/ui/img{{page.id}}/dag-part-2.png" style="width: 70%;" class="img-center">

(More on this later)

???
""  
In order to plan out our flow of data, let's whiteboard our fct_sales example.  
We'll start by considering the the sources that power our end result.   
The data sources we need are orders, products, customers, and stores.  

---

# Interface #1: The dbt DAG

Then add staging models: "The shape you wish your data came in"

<img src="/ui/img{{page.id}}/dag-part-3.png" style="width: 70%;" class="img-center">

???
Once we have our sources, we create our first layer of models which cleans and  
standardizes our raw data so that it's easier to work with downstream.  
  
We'll learn later on about what fills in our leftover question mark.  
""  

---

# Interface #1: The dbt DAG
### Model prefixes
.dense-text[
.left-column-33[
### stg_
- 1:1 with raw data tables
- **Source**-conformed
- Renaming, type casting, coalescing + nullif’ing
]

.right-column-66[
.left-column[
### dim_
- “Nouns”
- Core **business** entities
- Customers, products, apartments, providers, employees, ...
- Some "slowly-changing" attributes (e.g. address, email)
]

.right-column[
### fct_
- “Verbs”
- Core **business** processes
- Orders, cases, plays, listings, comments, ...
- Often built on top of an immutable event stream
]]]

???
""  
But first, let's talk about the prefixes you've seen so far on our model names.  
Having naming conventions for your models is a great way to leverage self-documentation  
to flag developers on model contents and purposes.  
  
At dbt Labs, here's what our prefixes mean:
- stg_ means that we're cleaning and standardizing raw data: 
  - We keep them 1:1 with our source tables, meaning that one model is cleaning
    one table - no joins.
  - We also don't aggregate here, we keep it source conformed - meaning
    that the grain of the data is kept the same.
  - Most of the SQL you'll see in these models are renaming, casting, coalescing,
    and null-if'ing. If it helps in downstream modeling, we'll also do things like case whens,
    row_numbers and filtering out duplicated or deleted records
    
- dim_ and fct_ are data sets that will be used in analysis. Typically  
these are surfaced to stakeholders or outside tooling (such as a BI tool).  
  - dim_ stands for dimension and usually describes an entity or object, such as  
    customers, products, apartments, or providers.
  - dimensions usually update by overwriting data.
  - fct_ describes something that's happening, i.e, an action or verb such as
    orders, cases, plays, or listings, or comments.
        - they usually append data rather than overwrite data, because the data  
        is logged as one row per action taken. 

[Teacher note:]
This says "typically surfaced to outside tooling", because you can also keep  
BI-type queries in dbt. A use case is Tableau, which does better when you  
pre-aggregate data sets before piping them in to reports. For this reason, you  
might create a "reports" folder with report_ prefixed models which do your joins  
and aggregations on top of the major concepts (dims and fcts).  
""  
---

<h3 style="text-align: center;"><span style="color: green;">source</span>
&rarr;
<span style="color: lightblue; font-size: 60%;">[base]</span>
&rarr;
<span style="color: blue;">stg</span>
&rarr;
<span style="color: grey; font-size: 60%;">[intermediate, lookup, utils]</span>
&rarr;
<span style="color: purple;">{fct</span>
+
<span style="color: navy;">dim}</span></h3>

<img src="/ui/img{{page.id}}/dag-grouped.png" style="width: 100%;">

???
""  
Here's an example of what a real DAG might end up looking like. You can  
see that we have:
- Our sources (the green nodes)
- Our staging models
- Some intermediate steps happening
- and our final dimensions and facts
""  

[Demo:]  
Draw circles/boxes around the different "model groups" (sources,
staging, intermediate, lookups, final fact/dim). Comment on how they can
be grouped together in all three interfaces.  

[Teacher Note:]  
If you start out by having the class focus on the major pieces (sourcse, stg_,  
dim_/fct_), you can then explain that the other layers are purely optional  
and serve to break down the flows into even more understandable steps.  
- base: usually used to bring together parts of a whole building block
- intermediate: usually used to break processes up in to logical steps 
or components towards the end goal.  

---
# Interface #1: The dbt DAG

Make sure that for each model, you ask:
- What does one record represent? Let's document this!
- What assumptions about our data are we making? We should test those!
- Am I using naming patterns? You should!

???
""  
Some good practices to go along with building your DAG are documentation,
testing, and organization. You can add more quality by asking these questions:
- What does one record represent? We should document this - this helps our team  
understand what they're working with.
- What assumptions about our data are we making? We should test these - this  
ensures that through our iterations that our assumptions remain true.
- Are we using naming patterns? We should! This helps us keep things organized
and understandable at a glance.

""  

---

# Interface #2: The repo

.left-column[
- **Within files:** adhere to a style guide
- **Between files:** better file organization = easier onboarding + better collaboration
]

.right-column[
.dense-text[
```
├── README.md
├── dbt_project.yml
└── models
    ├── marts
        └── core
            ├── core.yml
            └── dim_product.sql
            ├── dim_store.sql
            └── fact_sales.sql
    └── staging
        └── postgres
        └── shopify
            ├── shopify.yml
            ├── stg_shopify_customers.sql
            ├── stg_shopify_order_items.sql
            ├── stg_shopify_orders.sql
            └── stg_shopify_products.sql
```
]
]

???
""  
Our developers are doing most of their work within the files of the repository.  
We can keep this organized by separating our modeling layers in to dedicated  
folders. In this example, you can see that we have:
- A folder for our staging models and our marts models. Our staging folder is for
models that clean our source data, and our marts folder is for models that put 
together our cleaned data.
- Our staging folder contains subfolders for each source.  
- Within the subfolders, we have a our configurations and models that relate to
each source.
- Our marts folder contains a subfolder that relates to the area of business our
models apply to - right now we have core, but we could also add for others such
as product and marketing.
- Similar to our staging area, we keep configuration files within the folder of 
the models they seek to describe.
- And lastly, you can see that we've used our naming conventions for each model.
These names will show up as the object names in our warehouse and as the node
names in the DAG.

""  
---

# Interface #3: The Warehouse

.left-column-66[
- **Nomenclature.** How do we name our views + tables so they are consistent, easy to
find, and easy to infer contents?
- How do we organize our output objects? Same schema or different?
- What about the intermediate stuff we _don’t_ want to show?
- Who is your end user? Is it a person writing SQL, or a BI tool pointed to a production schema?
]

.right-column-33[
<img src="/ui/img{{page.id}}/mode-sidebar.png" style="width: 60%;" class="img-center">
]

???
""  
The last interface is the warehouse - which takes into consideration:
- How we've named things
- Our configurations for how materialized objects are organized
- and our permissioning setups in our warehouse

You can see on the right how a naming prefix has helped organize the models
into easy-to-find groups, and how the naming helps someone infer
the contents of the objects.
  
Depending on the usage of these objects, we might further customize them by  
configuring them to build into schemas by use case and setting permissions on  
those schemas based on who or what should use the objects within them.  

""  

---

class: subtitle, center, middle
# The way we roll

---

<img src="/ui/img{{page.id}}/discourse-stats.png" class="img-center">
&nbsp;
<img src="/ui/img{{page.id}}/discourse-intro.png" class="img-center">

.center[[Read it!](https://discourse.getdbt.com/t/how-we-structure-our-dbt-projects/355)]

???
""  
Here's an article we wrote on how we organize and structure our dbt projects.  
It's worth a read if you want to get some ideas for creating your own conventions!  
""  

---

class: subtitle

# Checkpoint:

- What are some helpful actions to do in the staging layer?  Why?
--

- For those who have worked with dim/fct tables, how do you think about the differences?
- Note: The difference can be a bit blurry if you are learning them for the first time.

--

## Questions?

---

class: subtitle, center, middle
# dbt + BI

## What BI tools do you use?

???
""  
Now we're going to talk about dbt artifacts and business intelligence tools.
Our end products typically get consumed by these, so it's important to understand  
the practices surrounding this part of the stack.  

""  

---

class: center
# What BI tools do you use?
<img src="/ui/img{{page.id}}/bi-logos-3.png" style="width: 40%;" class="img-center">

???
""  
There's a lot of BI tools at our disposal - tools such as Sisense, Metabase,  
Redash, Mode, Looker, Tableau, R, Chartio, etc.

Which of these tools do you use currently, or have you used in the past? 
Throw your answer in Slack! 
""  

[Teacher Notes:]  
Interactivity - have the class shout out which of these tools they've used in  
the slack channel!
---

# Which part of this should be in the dbt versus the BI tool?

<img src="/ui/img{{page.id}}/dag-part-3.png" style="width: 70%;" class="img-center">

???
""  
Let's go back to our question from earlier - where should our rollup query for  
sales by product and country live? Should it live in dbt, or in our BI tool?  
""  

---

# Which part of this should be in the dbt versus the BI tool?

For most orgs, the final rollup query should be in BI tool — allows for "drill down" functionality / further exploration.

???
""  
For most organizations, the final rollup query would be in the BI tool. By keeping  
things at the fct_sales level and aggregating using our BI tool's functionality,
we can allow for drill downs and deeper explorations.  

--

What if?
- This is a KPI your company reports on regularly?

???
And if this is a trusted KPI that your company reports on regularly?  

--

 Maybe you want it in a dbt model as well.
 
???
then you might decide to model this in dbt so that it can be version contolled  
as well! There's nothing wrong with moving important calculations in to dbt,  
but you might want to create a prefix for them to inform others that they  
are metrics aggregated from the major ideas.  

--

- You're building a prototype model or field?

???
what about if we're prototyping a new model or testing out transformations on  
a new peice of data?  

--

 Potentially build it in the BI tool first then "promote" it.

???
then we might want to build our prototyped query into the BI tool first, then  
move it into dbt once we verify that it's useful and agreed-upon.  

---


# What should you use your BI tool for?

???
""  
Queries and transformations can move around between dbt and the BI tool, so how  
do we try to stay effortful in what we decide to put where?  

Let's look at what we _should_ use our BI tool for:  
""  

---

## The things they've always been good at!

.left-column-33[
- User Interface
    - Point-and-click
    - Self-service filters, drill-downs
    - Accessible to users who may be less technical
]
.right-column-66[
.left-column[
- Viz!
    - Out-of-the-box
	- Color palettes ("my brand!")
    - Frameworks/libraries
]
.right-column[
- Nice-to-haves
    - Dynamic user input
    - Scheduling
	- Alerting
	- PDF’ing
	- CSV’ing
]
]

???
""  
They've always been good for supplying an easy-to-use user interface for less  
technical users, which include:
- point and click interfaces
- self-service filters and the ability for exploration through drill downs
  
They are also great at visualizing data. They usually have really great:
- out of the box visualizations that are easy to create
- the ability to brand the interface or visualizations using color palettes
- and the ability to bake in further functionality via imported frameworks and 
libraries
  
They also have some really nice features which make management overhead a little  
easier, such as:
- dynamic user input: allowing your users to dynamically change the inputs of a query  
- scheduling  
- alerting  
- and exporting to pdf's and csv's  

""  

---

## The things they're less good at

- Multilayered data transformation with complex ordered dependencies
- Enforcing a single sources of truth
    - Defining, documenting, centralizing business logic
- Maintaining separate data environments
    - Testing
    - Version control
- Running a lot non-performant SQL concurrently
- Managing user access to sensitive data

???
""  
They're not so great at:
- Multi-layered data transformations with complex ordered dependencies; For  
example, You need to transform data at different steps and you need to ensure  
everything happens in the correct order.
- Enforcing single sources of truth; For example, you need to ensure that    
logic for a sales calculation is the same anywhere it's used, and you also need  
to ensure the documentation and definitions of your data can be found in one  
place.
- Maintaining seperate data environments; For example, you need to ensure that  
you can test changes without messing up production, and need version control so  
so you can easily perform code comparisons or rollbacks.  
- Running a lot of non-performant SQL concurrently; For example, you need to make  
many calculations on large event tables before you can perform analyses on the data.  
- and managing user access to sensitive data; For example, you need to allow a certain  
group of stakeholders access to a certain set of models.   

""  

---

# What goes where?

.left-column[
## dbt layer
- Critical business logic
- Best agreed-on version of a model
- Complex SQL
- No wasted code! Everything is multipurpose*
- Anything worth version control
]

.right-column[
## BI layer
- Aggregations, calculated on the fly
- Joins qualified by user input
- Ad hoc queries and proto-models
- Select-star SQL that feeds R, Python, JS (custom viz)
- Nothing you can't take with you
]

???
""  
So, what goes where?  
  
Here's what we suggest:  
  
In dbt, we want critical business logic, the best agreed versions of models,  
complex SQL, queries that can serve multiple purposes, and really anything worth  
version control. You can think of this as our assembly line which produces 
expected outputs every time.  

In the BI layer, we want to keep aggregations and queries that need to be  
calculated on the fly, joins qualified by user input (throw back to dynamic user  
input), ad-hoc queries and prototyping, select stars that feed data science  
libraries, and _most importantly_: nothing you can't take with you! You can think  
of this as the place where we take the items produced from our assembly line  
and customize them to meet our stakeholder's needs.  
""  

[Teacher Note:]  
dbt models are like kitchen tools. The only single-purpose kitchen appliance
you should ever have is a rice cooker.

BI assets are like recipes. It doesn't hurt as much to have similar ones, but
when it comes time to make dinner, your guests might become confused :)

---

class: subtitle

# Checkpoint:

- What are the three interfaces into your dbt project?

???
""  
- What are the three interfaces into our dbt projects? [DAG, warehouse, repo]  

--

- What are some key distinctions between the dbt layer and BI layer that you want to keep in mind?

???
- What are some key distinctions between dbt and the BI layer that you should
keep in mind?  

--

- What naming conventions is your team already using?

???
- What are some naming conventions your team is already using?
""  

[Teacher Note:]  
Use your intuition on the interactivity of the group. If they're less outspoken,  
ask them to shout it out in the Slack channel. Otherwise, you can pause for a  
moment here to allow others to say what conventions they use!  

--

## Questions?

???
""  
Any questions?  
""  

---

class: subtitle

# Knowledge check

When creating a dbt project:
- First decide on the final artifact, and then sketch out the process for modeling it.
- Use the three interfaces to organize your models.
- Stage your raw source data before transforming it.
- Every model should have a primary key.
- Decide as an organization how to keep your BI layer thin and flexible.

???
""  
To recap:  

When creating a dbt project:
- First decide on the final artifact, and then sketch out the process for modeling it.
- Use the three interfaces to organize your models.
- Stage your raw source data before transforming it.
- Every model should have a primary key.
- Decide as an organization how to keep your BI layer thin and flexible.
  
""  

{% include options/last_slide.html %}

---

class: subtitle, middle, center
# Appendix: Words to the wise
### from one data modeler to another

???
This is a very special section, for anyone in the room who feels like the
presentation thus far has been too _descriptive_ ("what's hard about modeling?") and not
_prescriptive_ enough ("ok great, now _what should I do?_")

---

1. **Nomenclature** is _hard_, and it's worth your time to be consistent. It doesn't
matter if you prefer `fct_` or `fact_`, it's important that you choose one. (What's not hard?
Every model/object name is snake case + case insensitive.)
---
1. .grey[**Nomenclature.**]
2. **Public vs. Private.** Prefixed models with single underscores represent external/usable assets.  Double underscores represent internal/intermediate assets.
---
1. .grey[**Nomenclature.**]
2. .grey[**Public vs. Private.**]
3. **Modular model files:** Individual models should max out at a half-dozen
CTEs / few hundred lines of SQL. The CTEs should be named such that they narrate
the model's transformation. If one model file is getting too long, split it up
into multiple ephemeral models.
---
1. .grey[**Nomenclature.**]
2. .grey[**Public vs. Private.**]
3. .grey[**Modular model files.**]
4. **Macros in moderation:** Model code should seek to be _both_ DRY (Don't
Repeat Yourself) _and_ easily readable. Use Jinja + macros strongly and sparingly.
---
1. .grey[**Nomenclature.**]
2. .grey[**Public vs. Private.**]
3. .grey[**Modular model files.**]
4. .grey[**Macros in moderation.**]
5. **Reusable assets:** Final models (facts + dimensions) should be multipotent,
such that each can power an entire genre of potential queries. Your end user
or BI tool should be able to answer 80% of its most common questions by
querying **one (1)** table. Of the remaining 20%, no regularly running query
should require more than two join "steps." As soon as an ad hoc query is run
often, it should be refactored and modeled for accordingly.
---
1. .grey[**Nomenclature.**]
2. .grey[**Public vs. Private.**]
3. .grey[**Modular model files.**]
4. .grey[**Macros in moderation.**]
5. .grey[**Reusable assets.**]
6. **Documentation, documentation, documentation:** Your data models are your
product; the BI/query layer is your user experience. Having an untested,
undocumented model is worse than having no model at all.
<br>
<br>
_Corollary:_ Uncommented, unvetted, unreadable SQL—however clever
it may be—is _much_ worse than having no code at all.

---
1. .grey[**Nomenclature.**]
2. .grey[**Public vs. Private.**]
3. .grey[**Modular model files.**]
4. .grey[**Macros in moderation.**]
5. .grey[**Reusable assets.**]
6. .grey[**Documentation, documentation, documentation.**]
7. **Be your own best advocate:** Your dimensional modeling is your colleague's
feature engineering is your stakeholder's reconciliation of business definitions.
Analysts, BI users, business partners, and data scientists all can and should be
beneficiaries of the foundational work you're doing.
<br>
<br>
_Corollary:_ If they expend effort defining the same concepts on their own,
it's not only repeated work, but fertile ground for entropy at your organization.

???
I think about join "steps" as degrees of Kevin Bacon
---
1. .grey[**Nomenclature.**]
2. .grey[**Public vs. Private.**]
3. .grey[**Modular model files.**]
4. .grey[**Macros in moderation.**]
5. .grey[**Reusable assets.**]
6. .grey[**Documentation, documentation, documentation.**]
6. .grey[**Be your own best advocate.**]

# YMMV!
Take it in good health, and with many grains of salt.
