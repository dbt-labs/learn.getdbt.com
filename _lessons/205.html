---
layout: presentation
title: Data Warehouse Performance
lesson: 205
---

# Data Warehouse Performance

---

layout: false
# Q: Why are data warehouses fast?

--

### A: They're columnar!

???
Data warehouses _rose_ to prominence because they're columnar. Get it?

---

# Transactional database (row-level)

&nbsp;

| id | name    | favorite_color | is_creative |
|----|---------|----------------|-------------|
| 1  | Alice   | green          | false       |

&nbsp;

| id | name    | favorite_color | is_creative |
|----|---------|----------------|-------------|
| 2  | Barbara | blue           | true        |

---

# Analytic database (columnar)

&nbsp;

.left-column[
.left-column[
| id |
|----|   
| 1  |
| 2  |
]

.right-column[
| name    |
|---------|
| Alice   |
| Barbara |
]]

.right-column[
.left-column[
| favorite_color |
|----------------|
| green          |
| blue           |
]

.right-column[
| is_creative |
|-------------|
| false       |
| true        |
]]

---

# How you store <> how you query

| id | name    | favorite_color | is_creative |
|----|---------|----------------|-------------|
| 1  | Alice   | green          | false       |
| 2  | Barbara | blue           | true        |

```sql
select * from users
where id = 2
```

```sql
select

    is_creative,
    count(*)

from users
group by 1
```

---

# Q: Why are data warehouses fast?

### A: They're columnar!

--

Which allows them to <span style="background-color: yellow;">**limit scanned data**</span> when executing an analytical query.

--

### A: They're scalable.

---

# Horizontal scaling

<img src="/lessons/img/{{page.lesson}}/redshift-nodes.png" class="img-center">

---

# Vertical Scaling

.left-column[
<img src="/lessons/img/{{page.lesson}}/amd-vc.png" class="img-center">
]

.right-column[
<img src="/lessons/img/{{page.lesson}}/nvidia-gpu.png" class="img-center">
]

--

&nbsp;

<img src="/lessons/img/{{page.lesson}}/snowflake-scaling.png" class="img-center" style="width: 50%;">

---

# Q: Why are data warehouses fast?

### A: They're columnar!

Which allows them to <span style="background-color: yellow;">**limit scanned data**</span> when executing a query.

### A: They're scalable.

--

Data warehouses can benefit from <span style="background-color: yellow;">**horizontal and vertical scaling**</span>.

--

### Conclusion: They're optimized for analytics.

---

template: title
# Why are queries slow?

---

# Mental model

| Task                | Time  | Time (Relative) |
|---------------------|-------|-----------------|
| 1 CPU Cycle         | 0.3ns | 1 second        |
| Memory Access       | 120ns | 5 minutes       |
| Disk read           | 1ms   | 1 month         |
| Internet: SF to NYC | 40ms  | 4 years         |

---

# Conceptual framework

- Scanning data is **_slow_**
- Moving data around is **_slowww_**

- Warehouses are "logical" and "physical"
    - We like to think about the "logical" part of it
    - Sometimes it’s necessary to consider the "physical"

- Biggest takeaway?
    - Your time is valuable
    - Make choices that keep you focused on the "logical"

---

template: title
# Example: sorting data

---

<img src="/lessons/img/{{page.lesson}}/library.png" class="img-center">

???
1. Label each row with a letter. How many books start with the letter "D"?
2. Label each row with a letter. How many books were written in 2002?
3. Label each row with a year. How many books were written in 2002?

---

template: title
# I'm thinking of a number...

???
- I'm thinking of a number between 1 and 100. What is it?
- What's the quickest way to figure out what it is?
- Binary search: Is it greater than 50?
- This works _if and only if_ the numbers are **sorted**

---

# How to sort?

| order_id | user    | order_date | order_total |
|----------|---------|------------|-------------|
| 1        | Alice   | 2019-01-01 | $20.00      |
| 2        | Barbara | 2019-01-02 | $30.00      |
| 3        | Alice   | 2019-01-03 | $10.00      |
| ...      | ...     | ...        | ...         |

???
???
- You need a query before you know how to optimize for it.

--

```sql
select *
from orders
where order_date >= ‘2019-01-03’
```
???
- Which field should we sort on in order to optimize this query?

---

# In practice

Databases need to sort data for:
- Filters (`where`)
- Window functions (`partition by`, `order by`)

Databases need to move data around for:
- Uniqueness (`distinct`)
- Joins

???
Joins implicitly require finding all unique values for match/comparison, which
is why they're similar to getting `distinct` values.

---

# Queries are optimization problems

.left-column[
`dim_users`

| user_id | user_name | age |
|---------|-----------|-----|
| 1       | alice     | 27  |
| 2       | barbara   | 42  |
| ...     | ...       | ... |
]

.right-column[
`fct_orders`

| user_id | order_id | order_total |
|---------|----------|-------------|
| 1       | 123      | 75.00       |
| 2       | 124      | 30.00       |
| 1       | 125      | 40.00       |
| ...     | ...      | ...         |
]

--

&nbsp;

```sql
select sum(order_total)
from fct_users
join fct_orders using (user_id)
where age < 30
```

???
How to sort + distribute? Totally depends on the query you want to run! Here,
you should **sort on age** and **distribute on `user_id`**.

---

template: title
# Specific databases

---

# BigQuery partitioning

- BigQuery presents a totally different optimization problem!

- Limiting scanned data is not just a way to have faster queries, it's an
_economic necessity_.

    - BQ charges based on **total data scanned**, not complexity of computation
    - You can `partition` tables by a date, timestamp, or integer column, which
    is analogous to sorting on that column

---

# Snowflake clustering

Snowflake natively stores data in tiny chunks (“micropartitions”) and stores 
metadata about each partition, like the min/max value.

This improves performance dramatically by improving “query pruning”: How many 
micropartitions need to be scanned to return the results of my query?

---

<img src="/lessons/img/{{page.lesson}}/snowflake-pruned-query.png" class="img-center">

---

# When should we cluster?

Biggest datasets (event streams) are naturally ordered, so data will be 
distributed across micropartitions based on when it was collected.

When do we need to do more?
- On especially big tables. Snowflake says >1 TB, we say >100 GB.
- You’re regularly filtering on a field that is _not_ naturally sorted
- You're willing to pay more so that a particular query runs fastest
    - E.g. embedded reporting, exposed to external stakeholders

---

# Apples to apples

.left-column-33[
### Redshift
- Sort keys
    - Compound
    - Interleaved
- Dist style
    - Single column
    - Even
    - All
- Maintenance
    - vacuum
    - analyze
- Column compression
    - ZSTD
    - LZO
    - ...
]

.right-column-66[
.left-column[
### BigQuery
- Partitioning
- Clustering
- Nesting/arraying
]

.right-column[
### Snowflake
- Clustering (= sorting), only for large tables
]
]

???
Draw an arrow from left ("more work") to right ("less work"). A well tuned,
perfectly optimized Redshift cluster will execute queries _as quickly as_ a
comparable Snowflake cluster, for a lower price. But a poorly tuned Redshift
cluster is, well, a cluster.

---

class: bottom
background-color: orange
# On y va !
