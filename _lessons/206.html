---
layout: presentation
title: Big Data Technologies
lesson: 206
---

class: title, center, middle
# "Big Data" Technologies

---



# Apples to oranges...

| tech | stored as | storage/ compute | optimization | semi-structured data | "external" data | intuition |
|----------|-----------|------------------|-------------------------|----------------------|----------------------|-----------------------------------------------------|
| Postgres | rows | same | indexes | some support | via extensions? | everything works the way you expect |
| Redshift | columns | tightly coupled | sort + dist | minimal support | S3 via Spectrum | most things |

???
### Postgres
- Stores rows of structured, tabular data
- High concurrency, decent parallelization
- Compute and storage are tightly locked
- Performance optimization via indexes
- Decent support for semi-structured (JSON) data (as of version 9!)
- Shaky support for querying "foreign" data (via extensions)
- Everything works the way you expect
]

### Redshift
- Stores columns of structured, tabular data in proprietary file format
- Massively parallel query execution across multiple workers
- Compute and storage are interrelated
- Performance optimization via sorting, distribution
- Minimal support for semi- and unstructured data (JSON)
- Decent support for querying "external" data in S3 (via Spectrum)
- Most things work the way you expect

--
| Spark | files | separate | file formats + metadata | first-class support | all data is external | <span class="highlight">no guarantees</span> |

# ...to kumquats

???

### Spark
- "Data Lake": All data is stored externally
- Storage and compute are _completely_ separate. You pay for them independently.
- Performance optimization means storing in a columnar file format (parquet)
with excellent metadata
- Semi- and unstructured data is a first-class citizen, with pretty good SQL
functionality
- There's no guarantee that things will work the way you expect!


---

# Why not use?

### Spark, Presto, Athena, ...

- This a blog post I need to write :)
- They each have their fans, and their appropriate target audience!
- Basically:
    - They are incredibly powerful and cost-effective.
    - They're harder to use, and harder to reason about.

---

# What is a modern data warehouse?

It offers the best of both worlds:
    
- Intuitive **user experience** of a database: users, groups/roles,
authentication, permissions, information schema, everything is SQL

- Flexible and scalable **performance** of a data lake and distributed
processing technology

---

# Who is best in class?

- Snowflake + BigQuery. These are data lakes posing as data warehouses.
- Redshift is playing catch-up with Spectrum + RA3 nodes

### What sets them apart?
- Support for semi-structured data
- Support for external data
- Separation of compute + storage

---

# Semi-structured data

It's common that 90% of your data is tabular/structured, but there's a little
bit that makes more sense as JSON!

| bldg_id | location | occupancy | vacant_units | amenities |
|---------|-------------|-----------|--------------|----------------------------------------------------------------------------|
| 1 | Rittenhouse | 54 | `[2, 3, 9]` | `{"doorman": true, "pets_allowed": true, "pet_deposit": 1000}` |
| 2 | Fairmount | 8 | `[]` | `{ "yard_size_sqft": 100 }` |
| 3 | Austin, TX | 5 | `[1]` | `{ "yard_size_sqft": 1000, "pets_allowed": true }` |

vs.

| bldg_id | location | occupancy | vacant_unit_1 | num_vacant | doorman | pets_allowed | pet_deposit | yard_size_sqft |
|---------|-------------|-----------|--------------|----------------------------------------------------------------------------|
| 1 | Rittenhouse | 54 | 2 | 3 | true | true | 1000 | |
| 2 | Fairmount | 8 | | 0 | | | | 100 |
| 3 | Austin, TX | 5 | 1 | 1 | | true | | 1000 |

---

# Semi-structured data

| JSON support           | Redshift                         | Snowflake                           | BigQuery                            |
|------------------------|----------------------------------|-------------------------------------|-------------------------------------|
| Unnesting dictionaries | Kind of: `json_extract_path_text`| Yes definitely! `amenities:doorman` | Yes definitely! `amenities.doorman` |
| Flattening arrays      | Not natively                     | Yes: `lateral flatten`              | Yes: `cross join unnest`            |
| Loading data           | Flatten + unnest first           | Load as-is (`variant` blob)         | Load as repeated records            |


---

# External data

Can your database query files living in S3, Google Cloud Storage, Azure Blob storage...?

| External support           | Redshift                                       | Snowflake                                | BigQuery                                   |
|----------------------------|------------------------------------------------|------------------------------------------|--------------------------------------------|
| COPY from/to               | Yes, for S3                                    | Yes, for S3/GCS/Azure                    | Yes, can query GCS + G Drive               |
| Read from external tables  | Yes! Via Spectrum. Manual partitioning         | Yes! All via DDL, automatic partitioning | Yes! Powerful schema inference, but no DDL |
| External-only transform    | Limited cf. Athena                             | Extensive                                | Limited                                    |

.caption[Read more about [dbt + external tables](https://github.com/fishtown-analytics/dbt-external-tables)]

---

# Storage &harr; compute

When data is small and transformation is straightforward, you don't mind
that these are locked in a tight aspect ratio
- Everyone is sharing the same resources
- Need more? "I'd like another node, please!"

---

# Storage &harr; compute

What if you want more storage, without needing to pay for more compute nodes?
- Resources are scarce and analysts shouldn't compete with each other
- Event volume is _large_ and you can't store it all in-warehouse

What if you want it _now_?
- Scaling storage is easy: more files in S3/GCS/Blob
- Scaling compute is hard: bid for an EC2 spot instance
- Data is never stored "on disk," it is always read from file storage
    - Any caching is lost when compute spins down

---

# What if you have petabytes of data?

<img src="/lessons/img/{{page.lesson}}/jt-billion-rows.png" class="img-center">

---

# Different modeling paradigms
    
- All models are materialized incrementally
    - Replace discrete partitions, rather than merging/matching records*
    - Never `--full-refresh`
- Testing is relative
    - E.g. test that `unique` failures are < 1% of all rows
- Tables are "sharded" by a top-level identifier
    - region, account, client, etc.

---

# And if that data is constantly arriving?

You may have _two_ transformation workflows:

.left-column[
- (1) Calculate real-time directional signals on top of _streaming_ datasets
    - Kafka/Kinesis &rarr; KSQL
    - SparkSQL structured streaming
]

.right-column[
- Then, that data is funneled into S3/GCS/Blob for permanent storage, and staged 
in external tables or loaded into your warehouse, allowing you to...

- (2) Calculate all-time historical aggregates from batch incremental transformations
]

Oy. A lot of our friendly abstractions + assumptions are out the window...

---

class: subtitle, middle, center
# Speaking of modeling paradigms...

{% include options/next-presentation.html %}
