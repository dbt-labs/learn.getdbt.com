---
layout: presentation
title: Analytics Engineering 101
lesson: 101
---

* Review

Our viewpoint:

---
template: title
# Analytic code should be version controlled

---
layout: false
## What is version control?

- Version control helps manage changes to your code:
    - You can see history, along with context
    - Your changes have a review process for approval + collaboration
- Most teams use:
    - git for version control
    - GitHub to host their repo
    - Pull Requests for collaboration & approvals
- Some teams use complex [git flows](https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow)

---

## What does version control look like in traditional analytics?
- `orders_final_this_one_is_final.xls`
- Bookmarking the right query in your web editor

???
- Instead of `final final v20`, or just bookmarking a change
- Your future self will thank you
---

## What does version control look like with dbt?
- SQL is code, so…
- Exactly the same as software engineering! Here is an example.
- We have a [git guide](https://github.com/fishtown-analytics/corp/blob/master/git-guide.md) for those getting started

???
- SQL is code
- GitHub hosts things online (dropbox example)
- Show example Pull Request
- Literally exactly the same process as software engineers
- Extra resources:
    - Our git guide
---

* Analytics code should have quality assurance

* Analytics should be documented

---
template: title
# Analytics should be modular
---

## What is modularity?
- Each tool solves a separate class of problems, and can be swapped in and out.
- Modularity on various scales in software engineering:
    - Using Stripe for payments
    - Using a python package for shared problems
    - Within a codebase, use separate modules (== `.py` file)
    - Within a module (==`.py` file), abstract repeated code into functions
- Easier to define what each module does
- Allows you to focus on what makes your business unique

???
Modularity exists on many levels in software engineering. From the macro of which tools you use to solve a problem, to the micro of how you write your code.

---

## What does modularity look like in traditional analytics?

- Typically low modularity in the tech stack: using one system to cover many parts of your data pipeline. For example
    - Oracle as your data warehouse + ingestion tool
    - Looker as your BI + transformation tool
    - In-house tool for ETL
- Low modularity in queries: repeated snippets of SQL across many queries

???
Real life examples:
- Organizations that write the DML + DDL for every single transformation
- Really long queries that include business knowledge like "if cancelled_at is after shipped_at then refunded, else cancelled"

---

## What does modularity look like with dbt?

- The modern data stack: separate tools for ingestion, warehousing, transformation,
- Within a dbt project:
    - Using packages (more on this later)
    - Using separate models for different parts of transformation

???
From the pre-work: how we split up our code into separate "cleaning" and "transforming" models

---
template: title
# Analytics should use environments

---
layout: false
## What are environments?

Environments allow you to develop and test code without impacting the users of their software.

- **Production (prod):** the environment that end users interact with.  No one
can make changes in prod except for real human interaction/
- **Development (dev):** the environment engineers write code in
- **Staging / QA / CI / feature branches:** somewhere in between!

Releasing your code into production is called “deployment”.

---

## What do environments look like with dbt?

- Run dev transformations in your own schema (`dev_jcohen`), run prod transformations in a trusted schema.
- Usually use prod data for these transformations
- More info [here](https://docs.getdbt.com/docs/managing-environments).

---
template: title

# Analytics workflows should be automated

---
## What do workflows look like in traditional analytics?
1. Once a month: Run this query
2. Download the CSV
3. Paste the results into this Google Sheet
4. Copy and paste the charts into a board presentation

---
## What do workflows look like with dbt?
1. Data is ingested by Stitch/Fivetran
2. It is transformed with dbt and tested every x hours:
    - The latest version of the code is always used
    — Any errors are sent to a Slack channel
3. Your BI tool runs queries on the warehouse using the latest (transformed) data
