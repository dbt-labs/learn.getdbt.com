---
layout: presentation
title: Sources // Docs
lesson: 107
estimated_teaching_time: 30
teaching_method: live demo + slides. Questions encouraged.
working_session: true
---

# Sources

---

layout: false

# If any of the following is true...

* I'm switching my email marketing platform. (Or: I'm switching the tool that
extracts and loads _from_ my email marketing platform.) I want to know which 
final models depend on that source data.
--

* I'm switching from Redshift to Snowflake. All of my source data will be
in a different database/schema.
--

* Because no one is perfect, my data loading tools experience occasional 
hiccups or wrinkles that result in delayed or duplicated data
--

* My SQL transformations are working as expected, but my transformed data just 
doesn't look right. Help?!?!
--

* I want to stage & query data that lives in external file storage (!!)

---

# Sources may be right for you!

* Declare `database`, `schema`, and `table` names
* Test schema integrity (`unique`, `not_null`)
* Test loading freshness
* Declare `external` properties

---

# Familiar concepts

I define my sources in `stripe_source.yml`:
```yml
version: 2

sources:
  - name: stripe
    tables:
      - name: payment
```

And then I create a staging model, `stg_stripe_payment.sql`:

{% raw %}
```sql
with source as (

    select * from {{source('stripe', 'payment')}}

),

renamed as (

...
```
{% endraw %}

---

# Sources are green...

<img src="/lessons/img/107_1.png" class="img-center">

---

# And green is good

<img src="/lessons/img/107_2.png" class="img-center">

---

# A common approach

* Define all `sources` in YML files contained within a subfolder, `models/sources`.
* Daily jobs runs commands in the following order:
```bash
dbt test -m sources             # ensure no duplicates or unexpected nulls
dbt run                         # only runs if test above succeeds
dbt test --exclude sources
dbt source snapshot-freshness
```

---

template: title
# Docs

???
This will likely be split into a separate presentation, after we do
our renumbering

---

# Why is documentation important?

_Low quality research manifests as an environment of knowledge cacophony, 
where teams only read and trust research that they themselves created._
- Chetan Sharma and Jan Overgoor, [Scaling Knowledge at Airbnb](
https://medium.com/airbnb-engineering/scaling-knowledge-at-airbnb-875d73eff091)

### A lot of the reasons are self-explanatory...

---

# Analytics is the organization of an organization's information

* Analytics engineers are librarians
* Documentation is the card catalog
* The goal is to empower analyst-researchers
* _The libarian and the card catalog do not replace each otherâ€”they reinforce and enable each other!_

---

template: title
## How is knowledge documented and maintained at your company?

---

# Example: snowplow_sessions

{% raw %}
```yml
version: 2

models:
    - name: snowplow_sessions
      description: '{{ doc("snowplow_sessions") }}'
      
      columns:
          - name: session_id
            description: A visit / session identifier
            tests:
                - not_null
                - unique
                    
          - name: user_custom_id
            description: Unique ID set by business, user_id
                    
          - name: inferred_user_id
                    
          - name: exit_page_url
            description: The last page URL
            
        ...
```

---

# Example: snowplow_sessions

```md
# Snowplow Sessions
{% docs snowplow_sessions %}

Sessions are discrete periods of activity by a user on a website. In Snowplow, 
a session is typically defined as a series of activities followed by a 30-minute 
window without any activity. If the same cookie visits the website after 30 
minutes of inactivity, the javascript tracker will start a new session.

The Snowplow sessionization models _do not_ need to handle the determination of 
when to start a new session; that is handled in the Snowplow javascript tracker 
and saved in the `session_id` dimension of the event. The sessionization models 
build a `snowplow_sessions` model that aggregates a large number data points 
about the `session_id` that has been generated by the javascript tracker. Most 
of this information is calculated on top of page views from `snowplow_page_views`.

Note that the `inferred_user_id` field should be used as the primary user 
identifier in this model, as it has had "identity stitching" performed on it and 
can therefore reliably trace a user's history through time and across browsers/devices.

Please view the [Snowplow Canonical Web Data Model](https://github.com/snowplow/web-data-model#32-sessions-table) 
for more information on this model.

{% enddocs %}
```
{% endraw %}

---

<img src="/lessons/img/107_3.png" class="img-center">

&nbsp;

<img src="/lessons/img/107_4.png" class="img-center">
---
<img src="/lessons/img/107_5.png" class="img-center">
---

## dbt CLI
```bash
dbt docs generate
dbt docs serve
```

## dbt Cloud
<img src="/lessons/img/107_6.png" style="width: 90%; float: left;">

---

# Strong opinions

* Code is _surface area_. Testing and documentation are _coverage_.
* Untested, undocumented code _cannot be trusted_. In some sense, it is worse 
than having no code at all.
* Data documentation that seeks to accurately describe data transformation _must_
be updated as part of the same workflow as the transformation logic it seeks to 
describe. If they are separate, documentation will _always_ lag behind.

---

background-color: orange
class: middle, bottom

# Questions?
